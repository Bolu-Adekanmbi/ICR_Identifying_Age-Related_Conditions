{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf04da1",
   "metadata": {
    "papermill": {
     "duration": 0.009618,
     "end_time": "2023-08-09T15:02:12.656848",
     "exception": false,
     "start_time": "2023-08-09T15:02:12.647230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# model 1 -> lightgbm v2 of https://www.kaggle.com/code/mobassir/icr-lightgbm-model-baseline?scriptVersionId=134835182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27fa3d8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T15:02:12.677773Z",
     "iopub.status.busy": "2023-08-09T15:02:12.677250Z",
     "iopub.status.idle": "2023-08-09T15:02:13.967663Z",
     "shell.execute_reply": "2023-08-09T15:02:13.966585Z"
    },
    "papermill": {
     "duration": 1.303914,
     "end_time": "2023-08-09T15:02:13.970408",
     "exception": false,
     "start_time": "2023-08-09T15:02:12.666494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define balanced log loss function\n",
    "from sklearn.metrics import log_loss\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    nc = np.bincount(y_true)\n",
    "    return log_loss(y_true, y_pred, sample_weight=1/nc[y_true], eps=1e-15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a558828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T15:02:13.990564Z",
     "iopub.status.busy": "2023-08-09T15:02:13.990176Z",
     "iopub.status.idle": "2023-08-09T15:02:33.910303Z",
     "shell.execute_reply": "2023-08-09T15:02:33.909106Z"
    },
    "papermill": {
     "duration": 19.935475,
     "end_time": "2023-08-09T15:02:33.915095",
     "exception": false,
     "start_time": "2023-08-09T15:02:13.979620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################## bag: 0 ##########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 0, Fold: 0, Log Loss: 0.3310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 0, Fold: 1, Log Loss: 0.0966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 0, Fold: 2, Log Loss: 0.1476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 0, Fold: 3, Log Loss: 0.2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 0, Fold: 4, Log Loss: 0.1963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 0, Fold: 5, Log Loss: 0.2553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 0, Fold: 6, Log Loss: 0.2425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 556, Total valid: 61, Bags: 0, Fold: 7, Log Loss: 0.1347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 556, Total valid: 61, Bags: 0, Fold: 8, Log Loss: 0.4505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 556, Total valid: 61, Bags: 0, Fold: 9, Log Loss: 0.2076\n",
      "Average Log Loss for Bag 0: 0.2353\n",
      "########################## bag: 1 ##########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 1, Fold: 0, Log Loss: 0.4915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 1, Fold: 1, Log Loss: 0.1780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 1, Fold: 2, Log Loss: 0.1840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 1, Fold: 3, Log Loss: 0.1497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 1, Fold: 4, Log Loss: 0.2314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 1, Fold: 5, Log Loss: 0.1831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 1, Fold: 6, Log Loss: 0.2031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 556, Total valid: 61, Bags: 1, Fold: 7, Log Loss: 0.1907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 556, Total valid: 61, Bags: 1, Fold: 8, Log Loss: 0.1532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 556, Total valid: 61, Bags: 1, Fold: 9, Log Loss: 0.1800\n",
      "Average Log Loss for Bag 1: 0.2145\n",
      "########################## bag: 2 ##########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 2, Fold: 0, Log Loss: 0.2073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 2, Fold: 1, Log Loss: 0.1449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 2, Fold: 2, Log Loss: 0.1889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 2, Fold: 3, Log Loss: 0.1798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 2, Fold: 4, Log Loss: 0.2819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 2, Fold: 5, Log Loss: 0.4101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 2, Fold: 6, Log Loss: 0.4122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 556, Total valid: 61, Bags: 2, Fold: 7, Log Loss: 0.2325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 556, Total valid: 61, Bags: 2, Fold: 8, Log Loss: 0.1047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 556, Total valid: 61, Bags: 2, Fold: 9, Log Loss: 0.3961\n",
      "Average Log Loss for Bag 2: 0.2558\n",
      "########################## bag: 3 ##########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 3, Fold: 0, Log Loss: 0.1816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 3, Fold: 1, Log Loss: 0.3556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 3, Fold: 2, Log Loss: 0.2346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 3, Fold: 3, Log Loss: 0.2189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 3, Fold: 4, Log Loss: 0.1535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 3, Fold: 5, Log Loss: 0.1841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 3, Fold: 6, Log Loss: 0.1324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 556, Total valid: 61, Bags: 3, Fold: 7, Log Loss: 0.2666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 556, Total valid: 61, Bags: 3, Fold: 8, Log Loss: 0.4547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 556, Total valid: 61, Bags: 3, Fold: 9, Log Loss: 0.1330\n",
      "Average Log Loss for Bag 3: 0.2315\n",
      "########################## bag: 4 ##########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 4, Fold: 0, Log Loss: 0.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 4, Fold: 1, Log Loss: 0.1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 4, Fold: 2, Log Loss: 0.3344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 4, Fold: 3, Log Loss: 0.0839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 4, Fold: 4, Log Loss: 0.2082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 4, Fold: 5, Log Loss: 0.2133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 555, Total valid: 62, Bags: 4, Fold: 6, Log Loss: 0.4430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 556, Total valid: 61, Bags: 4, Fold: 7, Log Loss: 0.1259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 556, Total valid: 61, Bags: 4, Fold: 8, Log Loss: 0.2431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: 556, Total valid: 61, Bags: 4, Fold: 9, Log Loss: 0.3431\n",
      "Average Log Loss for Bag 4: 0.2226\n",
      "Average Log Loss after Full Training: 0.2319\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Class_0</th>\n",
       "      <th>Class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.361446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.361446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.361446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.361446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.361446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id   Class_0   Class_1\n",
       "0  00eed32682bb  0.638554  0.361446\n",
       "1  010ebe33f668  0.638554  0.361446\n",
       "2  02fa521e1838  0.638554  0.361446\n",
       "3  040e15f562a2  0.638554  0.361446\n",
       "4  046e85c7cc7f  0.638554  0.361446"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Define the number of bags and folds\n",
    "bag_num = 5\n",
    "n_fold = 10\n",
    "\n",
    "# Define the feature selection method\n",
    "k = 30  # Number of top features to select\n",
    "selector = SelectKBest(f_classif, k=k)\n",
    "\n",
    "\n",
    "# Define the competition log loss metric\n",
    "def competition_log_loss(y_true, y_pred):\n",
    "    N_0 = np.sum(1 - y_true)\n",
    "    N_1 = np.sum(y_true)\n",
    "    p_1 = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    p_0 = 1 - p_1\n",
    "    log_loss_0 = -np.sum((1 - y_true) * np.log(p_0)) / N_0\n",
    "    log_loss_1 = -np.sum(y_true * np.log(p_1)) / N_1\n",
    "    return (log_loss_0 + log_loss_1) / 2\n",
    "\n",
    "# Load the data\n",
    "COMP_PATH = \"/kaggle/input/icr-identify-age-related-conditions\"\n",
    "train = pd.read_csv(f\"{COMP_PATH}/train.csv\")\n",
    "test = pd.read_csv(f\"{COMP_PATH}/test.csv\")\n",
    "\n",
    "# Perform label encoding\n",
    "train['EJ'] = train['EJ'].map({'A': 0, 'B': 1})\n",
    "test['EJ'] = test['EJ'].map({'A': 0, 'B': 1})\n",
    "\n",
    "# Prepare the data\n",
    "df = train.copy()\n",
    "test_df = test.copy()\n",
    "feas_cols = [col for col in df.columns if col not in ['Id', 'Class']]\n",
    "\n",
    "# Define the imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply mean imputation on the data\n",
    "df[feas_cols] = imputer.fit_transform(df[feas_cols])\n",
    "test_df[feas_cols] = imputer.transform(test_df[feas_cols])\n",
    "\n",
    "# Define the LGBM parameters\n",
    "lgbm_params = {\n",
    "    \"boosting_type\": 'goss',\n",
    "    \"learning_rate\": 0.06733232950390658,\n",
    "    \"n_estimators\": 50000,\n",
    "    \"early_stopping_round\": 300,\n",
    "    \"random_state\": 118,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.6055755840633003,\n",
    "    \"class_weight\": 'balanced',\n",
    "    \"metric\": 'logloss',\n",
    "    \"is_unbalance\": True,\n",
    "    \"max_depth\": 12\n",
    "}\n",
    "\n",
    "# Initialize lists to store models and log losses\n",
    "models = []\n",
    "bag_log_losses = []\n",
    "feature_importance_df_total = pd.DataFrame()\n",
    "\n",
    "# Perform bagging and feature selection\n",
    "for bag in range(bag_num):\n",
    "    print(f'########################## bag: {bag} ##########################')\n",
    "    kf = StratifiedKFold(n_splits=n_fold, random_state=118*bag, shuffle=True)\n",
    "    fold_losses = []\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(df, df['Class'])):\n",
    "        train_df = df.iloc[train_idx]\n",
    "        valid_df = df.iloc[test_idx]\n",
    "        valid_ids = valid_df.Id.values.tolist()\n",
    "\n",
    "        X_train, y_train = train_df[feas_cols], train_df['Class']\n",
    "        X_valid, y_valid = valid_df[feas_cols], valid_df['Class']\n",
    "\n",
    "        # Perform feature selection\n",
    "        X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "        X_valid_selected = selector.transform(X_valid)\n",
    "        test_df_selected = selector.transform(test_df[feas_cols])\n",
    "\n",
    "        # Update feature columns\n",
    "        feas_cols_selected = [feas_cols[i] for i in selector.get_support(indices=True)]\n",
    "        feas_cols = feas_cols_selected\n",
    "\n",
    "        lgb = LGBMClassifier(**lgbm_params)\n",
    "        lgb.fit(X_train_selected, y_train, eval_set=(X_valid_selected, y_valid), verbose=False,\n",
    "                eval_metric=lambda y_true, y_pred: ('logloss', competition_log_loss(y_true, y_pred), False))\n",
    "\n",
    "        models.append(lgb)\n",
    "\n",
    "        # Calculate feature importances\n",
    "        feature_importances = lgb.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({'Feature': feas_cols, 'Importance': feature_importances})\n",
    "        feature_importance_df['Bag'] = bag\n",
    "        feature_importance_df['Fold'] = fold\n",
    "        feature_importance_df_total = pd.concat([feature_importance_df_total, feature_importance_df], axis=0)\n",
    "\n",
    "        y_pred = lgb.predict_proba(X_valid_selected)\n",
    "        fold_loss = log_loss(y_valid, y_pred)\n",
    "        fold_losses.append(fold_loss)\n",
    "        \n",
    "        print(f\"Total train: {len(train_df)}, Total valid: {len(valid_df)}, Bags: {bag}, Fold: {fold}, Log Loss: {fold_loss:.4f}\")\n",
    "    \n",
    "    avg_fold_loss = np.mean(fold_losses)\n",
    "    bag_log_losses.append(avg_fold_loss)\n",
    "    print(f\"Average Log Loss for Bag {bag}: {avg_fold_loss:.4f}\")\n",
    "\n",
    "avg_loss = np.mean(bag_log_losses)\n",
    "print(f\"Average Log Loss after Full Training: {avg_loss:.4f}\")\n",
    "\n",
    "# Calculate weights based on inverse of log loss\n",
    "weights = [1 / loss for loss in bag_log_losses]\n",
    "total_weight = sum(weights)\n",
    "weights = [weight / total_weight for weight in weights]\n",
    "\n",
    "# Prepare submission dataframe\n",
    "lgbm_preds = np.zeros(len(test_df))\n",
    "for bag, weight in zip(range(bag_num), weights):\n",
    "    for fold in range(n_fold):\n",
    "        clf = models[bag * n_fold + fold]\n",
    "        lgbm_preds += weight * clf.predict_proba(test_df_selected)[:, 1] / n_fold\n",
    "\n",
    "lgbm = test_df[['Id']].copy()\n",
    "lgbm['Class_0'] = 1 - lgbm_preds\n",
    "lgbm['Class_1'] = lgbm_preds\n",
    "lgbm.to_csv('lgbm0.16.csv', index=False)\n",
    "lgbm.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a29127",
   "metadata": {
    "papermill": {
     "duration": 0.01856,
     "end_time": "2023-08-09T15:02:33.954347",
     "exception": false,
     "start_time": "2023-08-09T15:02:33.935787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# model 2 -> Stacked XGB Models --> v13 https://www.kaggle.com/code/zhukovoleksiy/icr-stacking-xgb-models?scriptVersionId=136494925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55501244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T15:02:33.992831Z",
     "iopub.status.busy": "2023-08-09T15:02:33.992429Z",
     "iopub.status.idle": "2023-08-09T15:02:35.471951Z",
     "shell.execute_reply": "2023-08-09T15:02:35.470211Z"
    },
    "papermill": {
     "duration": 1.504082,
     "end_time": "2023-08-09T15:02:35.476691",
     "exception": false,
     "start_time": "2023-08-09T15:02:33.972609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape: (617, 57)\n",
      "\n",
      "df_test shape: (5, 56)\n",
      "\n",
      "No. of records with missing value in Train data set after Imputation : 0\n",
      "No. of records with missing value in Test data set after Imputation : 0\n",
      "==================================================\n",
      "Shape of the Train data set : (617, 56)\n",
      "Shape of the Test data set : (5, 56)\n",
      "X_train shape :(617, 56) , y_train shape :(617,)\n",
      "X_test shape :(5, 56)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "import random\n",
    "import gc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Import sklearn classes for model selection, cross validation, and performance evaluation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "from category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Import libraries for Hypertuning\n",
    "import optuna\n",
    "\n",
    "# Import libraries for gradient boosting\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb \n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\n",
    "from catboost import Pool\n",
    "\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Useful line of code to set the display option so we could see all the columns in pd dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "filepath = '/kaggle/input/icr-identify-age-related-conditions/'\n",
    "df_train = pd.read_csv(os.path.join(filepath, 'train.csv'), index_col=[0])\n",
    "df_test = pd.read_csv(os.path.join(filepath, 'test.csv'), index_col=[0])\n",
    "greeks = pd.read_csv(f'{filepath}greeks.csv')\n",
    "\n",
    "target_col = 'Class'\n",
    "\n",
    "df_train['EJ'].replace({'A':0, 'B':1}, inplace=True)\n",
    "df_test['EJ'].replace({'A':0, 'B':1}, inplace=True)\n",
    "\n",
    "# df_train = df_train.drop(['EJ'], axis=1)\n",
    "# df_test = df_test.drop(['EJ'], axis=1)\n",
    "\n",
    "df_train = df_train.rename(columns={'BD ': 'BD', 'CD ': 'CD', 'CW ': 'CW', 'FD ': 'FD'})\n",
    "df_test = df_test.rename(columns={'BD ': 'BD', 'CD ': 'CD', 'CW ': 'CW', 'FD ': 'FD'})\n",
    "\n",
    "print(f'df_train shape: {df_train.shape}\\n')\n",
    "print(f'df_test shape: {df_test.shape}\\n')\n",
    "# Only include numerical features\n",
    "df_train_numerical = df_train.drop(['Class'], axis=1)\n",
    "# Initialize the KNNImputer with the desired number of neighbors\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Perform KNN imputation\n",
    "df_train_imputed = pd.DataFrame(imputer.fit_transform(df_train[df_train_numerical.columns]), columns=df_train_numerical.columns)\n",
    "df_test_imputed =pd.DataFrame(imputer.transform(df_test[df_train_numerical.columns]), columns=df_train_numerical.columns)\n",
    "\n",
    "# Check if there are still missing values in the train and test data sets\n",
    "df_train_null = df_train_imputed[df_train_imputed.isnull().any(axis=1)]\n",
    "df_test_null = df_test_imputed[df_test_imputed.isnull().any(axis=1)]\n",
    "\n",
    "# Display the rows with null values\n",
    "print('No. of records with missing value in Train data set after Imputation : {}'.format(df_train_null.shape[0]))\n",
    "print('No. of records with missing value in Test data set after Imputation : {}'.format(df_test_null.shape[0]))\n",
    "\n",
    "print('=' * 50)\n",
    "\n",
    "# Replace the imputed columns in the train data sets\n",
    "df_train_2 = df_train.drop(df_train_numerical.columns, axis=1).reset_index()\n",
    "df_train_2 = pd.concat([df_train_2, df_train_imputed], axis=1)\n",
    "\n",
    "# Replace the imputed columns in the test data sets\n",
    "df_test_2 = df_test.drop(df_train_numerical.columns, axis=1).reset_index()\n",
    "df_test_2 = pd.concat([df_test_2, df_test_imputed], axis=1)\n",
    "\n",
    "X_train = df_train_2.drop([f'{target_col}', 'Id'],axis=1).reset_index(drop=True)\n",
    "y_train = df_train_2[f'{target_col}'].reset_index(drop=True)\n",
    "X_test = df_test_2.drop(['Id'],axis=1).reset_index(drop=True)\n",
    "\n",
    "# Check the shape of the train and test data set \n",
    "print('Shape of the Train data set : {}'.format(X_train.shape))\n",
    "print('Shape of the Test data set : {}'.format(X_test.shape))\n",
    "\n",
    "numeric_columns = [_ for _ in X_train.columns if _ not in ['EJ']]\n",
    "log_cols = [_ for _ in X_train.columns if _ not in ['EJ', 'BN', 'CW', 'EL', 'GL']]\n",
    "X_train.loc[:, log_cols] = np.log1p(X_train.loc[:, log_cols])\n",
    "X_test.loc[:, log_cols] = np.log1p(X_test.loc[:, log_cols])\n",
    "sc = StandardScaler() # MinMaxScaler or StandardScaler\n",
    "X_train[numeric_columns] = sc.fit_transform(X_train[numeric_columns])\n",
    "X_test[numeric_columns] = sc.transform(X_test[numeric_columns])\n",
    "\n",
    "print(f\"X_train shape :{X_train.shape} , y_train shape :{y_train.shape}\")\n",
    "print(f\"X_test shape :{X_test.shape}\")\n",
    "\n",
    "# Delete the train and test dataframes to free up memory\n",
    "del df_train, df_test, df_train_imputed, df_train_2, df_test_2, df_train_null, df_test_null\n",
    "\n",
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "class_weight_0 = 1.0\n",
    "class_weight_1 = 1.0 / scale_pos_weight\n",
    "\n",
    "class_weights_cat = [class_weight_0, class_weight_1]\n",
    "\n",
    "class_weights_lgb = {0: class_weight_0, 1: class_weight_1}\n",
    "def calc_log_loss_weight(y_true):\n",
    "    nc = np.bincount(y_true)\n",
    "    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n",
    "    return w0, w1\n",
    "\n",
    "\n",
    "class Splitter:\n",
    "    def __init__(self, kfold=True, n_splits=5, greeks=pd.DataFrame()):\n",
    "        self.n_splits = n_splits\n",
    "        self.kfold = kfold\n",
    "        self.greeks = greeks\n",
    "\n",
    "    def split_data(self, X, y, random_state_list):\n",
    "        if self.kfold == 'skf':\n",
    "            for random_state in random_state_list:\n",
    "                kf = StratifiedKFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n",
    "                for train_index, val_index in kf.split(X, y):\n",
    "                    if type(X) is np.ndarray:\n",
    "                        X_train, X_val = X[train_index], X[val_index]\n",
    "                        y_train, y_val = y[train_index], y[val_index]\n",
    "                    else:\n",
    "                        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "                        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "                    yield X_train, X_val, y_train, y_val\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid kfold: Must be True\")\n",
    "class Classifier:\n",
    "    def __init__(self, n_estimators=100, device=\"cpu\", random_state=42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.device = device\n",
    "        self.random_state = random_state\n",
    "        self.models = self._define_model()\n",
    "        self.models_name = list(self._define_model().keys())\n",
    "        self.len_models = len(self.models)\n",
    "        \n",
    "    def _define_model(self):\n",
    "        \n",
    "        xgb_optuna1 = {\n",
    "            'n_estimators': 900,\n",
    "            'learning_rate': 0.09641232707445854,\n",
    "            'booster': 'gbtree',\n",
    "            'lambda': 4.666002223704784,\n",
    "            'alpha': 3.708175990751336,\n",
    "            'subsample': 0.6100174145229473,\n",
    "            'colsample_bytree': 0.5506821152321051,\n",
    "            'max_depth': 7,\n",
    "            'min_child_weight': 3,\n",
    "            'eta': 1.740374368661041,\n",
    "            'gamma': 0.007427363662926455,\n",
    "            'grow_policy': 'depthwise',\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'logloss',\n",
    "            'verbosity': 0,\n",
    "            'random_state': self.random_state,\n",
    "            'scale_pos_weight': scale_pos_weight\n",
    "        }\n",
    "        \n",
    "        xgb_optuna2 = {\n",
    "            'n_estimators': 650,\n",
    "            'learning_rate': 0.012208383405206188,\n",
    "            'booster': 'gbtree',\n",
    "            'lambda': 0.009968756668882757,\n",
    "            'alpha': 0.02666266827121168,\n",
    "            'subsample': 0.7097814108897231,\n",
    "            'colsample_bytree': 0.7946945784285216,\n",
    "            'max_depth': 3,\n",
    "            'min_child_weight': 4,\n",
    "            'eta': 0.5480204506554545,\n",
    "            'gamma': 0.8788654128774149,\n",
    "            'scale_pos_weight': 4.71,\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'logloss',\n",
    "            'verbosity': 0,\n",
    "            'random_state': self.random_state,\n",
    "            'scale_pos_weight': scale_pos_weight\n",
    "        }\n",
    "\n",
    "        xgb_params = {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'learning_rate': 0.413327571405248,\n",
    "            'booster': 'gbtree',\n",
    "            'lambda': 0.0000263894617720096,\n",
    "            'alpha': 0.000463768723479341,\n",
    "            'subsample': 0.237467672874133,\n",
    "            'colsample_bytree': 0.618829300507829,\n",
    "            'max_depth': 5,\n",
    "            'min_child_weight': 9,\n",
    "            'eta': 2.09477807126539E-06,\n",
    "            'gamma': 0.000847289463422307,\n",
    "            'grow_policy': 'depthwise',\n",
    "            'n_jobs': -1,\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'logloss',\n",
    "            'scale_pos_weight': scale_pos_weight,\n",
    "            'verbosity': 0,\n",
    "            'random_state': self.random_state,\n",
    "            \n",
    "        }\n",
    "        \n",
    "        xgb_params2 = {\n",
    "            'colsample_bytree': 0.5646751146007976,\n",
    "            'gamma': 7.788727238356553e-06,\n",
    "            'learning_rate': 0.1419865761603358,\n",
    "            'max_bin': 824,\n",
    "            'min_child_weight': 1,\n",
    "            'random_state': 811996,\n",
    "            'reg_alpha': 1.6259583347890365e-07,\n",
    "            'reg_lambda': 2.110691851528507e-08,\n",
    "            'subsample': 0.879020578464637,\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'logloss',\n",
    "            'max_depth': 3,\n",
    "            'n_jobs': -1,\n",
    "            'verbosity': 0,\n",
    "            'random_state': self.random_state,\n",
    "            'scale_pos_weight': scale_pos_weight\n",
    "        }\n",
    "        \n",
    "        xgb_params3 = {\n",
    "            'random_state': self.random_state,\n",
    "            'colsample_bytree': 0.4836462317215041,\n",
    "            'eta': 0.05976752607337169,\n",
    "            'gamma': 1,\n",
    "            'lambda': 0.2976432557733288,\n",
    "            'max_depth': 6,\n",
    "            'min_child_weight': 1,\n",
    "            'n_estimators': 550,\n",
    "            'objective': 'binary:logistic',\n",
    "            'scale_pos_weight': 4.260162886376033,\n",
    "            'subsample': 0.7119282378433924,\n",
    "        }\n",
    "        \n",
    "        xgb_params4 = {\n",
    "            'colsample_bytree': 0.8757972257439255,\n",
    "            'gamma': 0.11135738771999848,\n",
    "            'max_depth': 7,\n",
    "            'min_child_weight': 3,\n",
    "            'reg_alpha': 0.4833998914998038,\n",
    "            'reg_lambda': 0.006223568555619563,\n",
    "            'scale_pos_weight': 8,\n",
    "            'subsample': 0.7056434340275685,\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "        \n",
    "        xgb_params5 = {\n",
    "            'max_depth': 5, \n",
    "            'min_child_weight': 2.934487833919741,\n",
    "            'learning_rate': 0.11341944575807082, \n",
    "            'subsample': 0.9045063514419968,\n",
    "            'gamma': 0.4329153382843715,\n",
    "            'colsample_bytree': 0.38872702868412506,\n",
    "            'colsample_bylevel': 0.8321880031718571,\n",
    "            'colsample_bynode': 0.802355707802605,\n",
    "            'random_state': self.random_state\n",
    "       }\n",
    "        \n",
    "        if self.device == 'gpu':\n",
    "            xgb_params['tree_method'] = 'gpu_hist'\n",
    "            xgb_params['predictor'] = 'gpu_predictor'\n",
    "       \n",
    "        models = {\n",
    "            'xgb01': xgb.XGBClassifier(**xgb_optuna1),\n",
    "            'xgb02': xgb.XGBClassifier(**xgb_optuna2),\n",
    "            'xgb1': xgb.XGBClassifier(**xgb_params),\n",
    "            'xgb2': xgb.XGBClassifier(**xgb_params2),\n",
    "            'xgb3': xgb.XGBClassifier(**xgb_params3),\n",
    "            #'xgb4': xgb.XGBClassifier(**xgb_params4),\n",
    "            'xgb5': xgb.XGBClassifier(**xgb_params5),\n",
    "            #add some models with default params to \"simplify\" ensemble\n",
    "            'svc': SVC(random_state=self.random_state, probability=True),\n",
    "            'brf': BalancedRandomForestClassifier(random_state=self.random_state),\n",
    "            #'lr': LogisticRegression(random_state=self.random_state)\n",
    "        }\n",
    "        \n",
    "        return models\n",
    "\n",
    "class OptunaWeights:\n",
    "    def __init__(self, random_state, n_trials=1000):\n",
    "        self.study = None\n",
    "        self.weights = None\n",
    "        self.random_state = random_state\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def _objective(self, trial, y_true, y_preds):\n",
    "        # Define the weights for the predictions from each model\n",
    "        weights = [trial.suggest_float(f\"weight{n}\", 1e-14, 1) for n in range(len(y_preds))]\n",
    "\n",
    "        # Calculate the weighted prediction\n",
    "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n",
    "\n",
    "        # Calculate the score for the weighted prediction\n",
    "        # score = log_loss(y_true, weighted_pred)\n",
    "        score = balanced_log_loss(y_true, weighted_pred)\n",
    "        return score\n",
    "\n",
    "    def fit(self, y_true, y_preds):\n",
    "        optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n",
    "        pruner = optuna.pruners.HyperbandPruner()\n",
    "        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='minimize')\n",
    "        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n",
    "        self.study.optimize(objective_partial, n_trials=self.n_trials)\n",
    "        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n",
    "\n",
    "    def predict(self, y_preds):\n",
    "        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n",
    "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n",
    "        return weighted_pred\n",
    "\n",
    "    def fit_predict(self, y_true, y_preds):\n",
    "        self.fit(y_true, y_preds)\n",
    "        return self.predict(y_preds)\n",
    "    \n",
    "    def weights(self):\n",
    "        return self.weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2fca55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T15:02:35.525977Z",
     "iopub.status.busy": "2023-08-09T15:02:35.525528Z",
     "iopub.status.idle": "2023-08-09T15:11:26.870418Z",
     "shell.execute_reply": "2023-08-09T15:11:26.868582Z"
    },
    "papermill": {
     "duration": 531.367384,
     "end_time": "2023-08-09T15:11:26.873167",
     "exception": false,
     "start_time": "2023-08-09T15:02:35.505783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb01 [FOLD-0 SEED-1824] Recall score: 0.86364\n",
      "xgb01 [FOLD-0 SEED-1824] Precision score: 0.76000\n",
      "xgb01 [FOLD-0 SEED-1824] BalancedLogLoss score: 0.21750\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-0 SEED-1824] Recall score: 0.81818\n",
      "xgb02 [FOLD-0 SEED-1824] Precision score: 0.66667\n",
      "xgb02 [FOLD-0 SEED-1824] BalancedLogLoss score: 0.25142\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-0 SEED-1824] Recall score: 0.86364\n",
      "xgb1 [FOLD-0 SEED-1824] Precision score: 0.67857\n",
      "xgb1 [FOLD-0 SEED-1824] BalancedLogLoss score: 0.28585\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-0 SEED-1824] Recall score: 0.90909\n",
      "xgb2 [FOLD-0 SEED-1824] Precision score: 0.71429\n",
      "xgb2 [FOLD-0 SEED-1824] BalancedLogLoss score: 0.23401\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-0 SEED-1824] Recall score: 0.77273\n",
      "xgb3 [FOLD-0 SEED-1824] Precision score: 0.80952\n",
      "xgb3 [FOLD-0 SEED-1824] BalancedLogLoss score: 0.25061\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-0 SEED-1824] Recall score: 0.77273\n",
      "xgb5 [FOLD-0 SEED-1824] Precision score: 0.80952\n",
      "xgb5 [FOLD-0 SEED-1824] BalancedLogLoss score: 0.26587\n",
      "--------------------------------------------------\n",
      "svc [FOLD-0 SEED-1824] Recall score: 0.77273\n",
      "svc [FOLD-0 SEED-1824] Precision score: 0.94444\n",
      "svc [FOLD-0 SEED-1824] BalancedLogLoss score: 0.29730\n",
      "--------------------------------------------------\n",
      "brf [FOLD-0 SEED-1824] Recall score: 0.95455\n",
      "brf [FOLD-0 SEED-1824] Precision score: 0.65625\n",
      "brf [FOLD-0 SEED-1824] BalancedLogLoss score: 0.34995\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-0 SEED-1824] BalancedLogLoss score 0.20997\n",
      "==================================================\n",
      "xgb01 [FOLD-1 SEED-1824] Recall score: 0.86364\n",
      "xgb01 [FOLD-1 SEED-1824] Precision score: 0.73077\n",
      "xgb01 [FOLD-1 SEED-1824] BalancedLogLoss score: 0.22865\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-1 SEED-1824] Recall score: 0.90909\n",
      "xgb02 [FOLD-1 SEED-1824] Precision score: 0.80000\n",
      "xgb02 [FOLD-1 SEED-1824] BalancedLogLoss score: 0.18755\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-1 SEED-1824] Recall score: 0.90909\n",
      "xgb1 [FOLD-1 SEED-1824] Precision score: 0.68966\n",
      "xgb1 [FOLD-1 SEED-1824] BalancedLogLoss score: 0.28282\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-1 SEED-1824] Recall score: 0.90909\n",
      "xgb2 [FOLD-1 SEED-1824] Precision score: 0.83333\n",
      "xgb2 [FOLD-1 SEED-1824] BalancedLogLoss score: 0.20455\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-1 SEED-1824] Recall score: 0.81818\n",
      "xgb3 [FOLD-1 SEED-1824] Precision score: 0.90000\n",
      "xgb3 [FOLD-1 SEED-1824] BalancedLogLoss score: 0.21510\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-1 SEED-1824] Recall score: 0.77273\n",
      "xgb5 [FOLD-1 SEED-1824] Precision score: 0.80952\n",
      "xgb5 [FOLD-1 SEED-1824] BalancedLogLoss score: 0.29620\n",
      "--------------------------------------------------\n",
      "svc [FOLD-1 SEED-1824] Recall score: 0.81818\n",
      "svc [FOLD-1 SEED-1824] Precision score: 0.85714\n",
      "svc [FOLD-1 SEED-1824] BalancedLogLoss score: 0.29739\n",
      "--------------------------------------------------\n",
      "brf [FOLD-1 SEED-1824] Recall score: 0.95455\n",
      "brf [FOLD-1 SEED-1824] Precision score: 0.65625\n",
      "brf [FOLD-1 SEED-1824] BalancedLogLoss score: 0.35990\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-1 SEED-1824] BalancedLogLoss score 0.16891\n",
      "==================================================\n",
      "xgb01 [FOLD-2 SEED-1824] Recall score: 0.90909\n",
      "xgb01 [FOLD-2 SEED-1824] Precision score: 0.57143\n",
      "xgb01 [FOLD-2 SEED-1824] BalancedLogLoss score: 0.24177\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-2 SEED-1824] Recall score: 0.90909\n",
      "xgb02 [FOLD-2 SEED-1824] Precision score: 0.62500\n",
      "xgb02 [FOLD-2 SEED-1824] BalancedLogLoss score: 0.23533\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-2 SEED-1824] Recall score: 0.72727\n",
      "xgb1 [FOLD-2 SEED-1824] Precision score: 0.41026\n",
      "xgb1 [FOLD-2 SEED-1824] BalancedLogLoss score: 0.49696\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-2 SEED-1824] Recall score: 0.86364\n",
      "xgb2 [FOLD-2 SEED-1824] Precision score: 0.65517\n",
      "xgb2 [FOLD-2 SEED-1824] BalancedLogLoss score: 0.22591\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-2 SEED-1824] Recall score: 0.81818\n",
      "xgb3 [FOLD-2 SEED-1824] Precision score: 0.75000\n",
      "xgb3 [FOLD-2 SEED-1824] BalancedLogLoss score: 0.25860\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-2 SEED-1824] Recall score: 0.86364\n",
      "xgb5 [FOLD-2 SEED-1824] Precision score: 0.86364\n",
      "xgb5 [FOLD-2 SEED-1824] BalancedLogLoss score: 0.29544\n",
      "--------------------------------------------------\n",
      "svc [FOLD-2 SEED-1824] Recall score: 0.86364\n",
      "svc [FOLD-2 SEED-1824] Precision score: 0.70370\n",
      "svc [FOLD-2 SEED-1824] BalancedLogLoss score: 0.37921\n",
      "--------------------------------------------------\n",
      "brf [FOLD-2 SEED-1824] Recall score: 0.90909\n",
      "brf [FOLD-2 SEED-1824] Precision score: 0.57143\n",
      "brf [FOLD-2 SEED-1824] BalancedLogLoss score: 0.39096\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-2 SEED-1824] BalancedLogLoss score 0.22472\n",
      "==================================================\n",
      "xgb01 [FOLD-3 SEED-1824] Recall score: 0.85714\n",
      "xgb01 [FOLD-3 SEED-1824] Precision score: 0.69231\n",
      "xgb01 [FOLD-3 SEED-1824] BalancedLogLoss score: 0.30486\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-3 SEED-1824] Recall score: 0.90476\n",
      "xgb02 [FOLD-3 SEED-1824] Precision score: 0.70370\n",
      "xgb02 [FOLD-3 SEED-1824] BalancedLogLoss score: 0.33509\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-3 SEED-1824] Recall score: 0.90476\n",
      "xgb1 [FOLD-3 SEED-1824] Precision score: 0.51351\n",
      "xgb1 [FOLD-3 SEED-1824] BalancedLogLoss score: 0.34022\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-3 SEED-1824] Recall score: 0.95238\n",
      "xgb2 [FOLD-3 SEED-1824] Precision score: 0.68966\n",
      "xgb2 [FOLD-3 SEED-1824] BalancedLogLoss score: 0.30241\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-3 SEED-1824] Recall score: 0.76190\n",
      "xgb3 [FOLD-3 SEED-1824] Precision score: 0.72727\n",
      "xgb3 [FOLD-3 SEED-1824] BalancedLogLoss score: 0.35682\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-3 SEED-1824] Recall score: 0.66667\n",
      "xgb5 [FOLD-3 SEED-1824] Precision score: 0.70000\n",
      "xgb5 [FOLD-3 SEED-1824] BalancedLogLoss score: 0.36452\n",
      "--------------------------------------------------\n",
      "svc [FOLD-3 SEED-1824] Recall score: 0.52381\n",
      "svc [FOLD-3 SEED-1824] Precision score: 0.73333\n",
      "svc [FOLD-3 SEED-1824] BalancedLogLoss score: 0.60580\n",
      "--------------------------------------------------\n",
      "brf [FOLD-3 SEED-1824] Recall score: 0.95238\n",
      "brf [FOLD-3 SEED-1824] Precision score: 0.64516\n",
      "brf [FOLD-3 SEED-1824] BalancedLogLoss score: 0.39855\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-3 SEED-1824] BalancedLogLoss score 0.28058\n",
      "==================================================\n",
      "xgb01 [FOLD-4 SEED-1824] Recall score: 0.76190\n",
      "xgb01 [FOLD-4 SEED-1824] Precision score: 0.64000\n",
      "xgb01 [FOLD-4 SEED-1824] BalancedLogLoss score: 0.30328\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-4 SEED-1824] Recall score: 0.80952\n",
      "xgb02 [FOLD-4 SEED-1824] Precision score: 0.60714\n",
      "xgb02 [FOLD-4 SEED-1824] BalancedLogLoss score: 0.29553\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-4 SEED-1824] Recall score: 0.95238\n",
      "xgb1 [FOLD-4 SEED-1824] Precision score: 0.62500\n",
      "xgb1 [FOLD-4 SEED-1824] BalancedLogLoss score: 0.37180\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-4 SEED-1824] Recall score: 0.76190\n",
      "xgb2 [FOLD-4 SEED-1824] Precision score: 0.66667\n",
      "xgb2 [FOLD-4 SEED-1824] BalancedLogLoss score: 0.28601\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-4 SEED-1824] Recall score: 0.76190\n",
      "xgb3 [FOLD-4 SEED-1824] Precision score: 0.76190\n",
      "xgb3 [FOLD-4 SEED-1824] BalancedLogLoss score: 0.33533\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-4 SEED-1824] Recall score: 0.76190\n",
      "xgb5 [FOLD-4 SEED-1824] Precision score: 0.84211\n",
      "xgb5 [FOLD-4 SEED-1824] BalancedLogLoss score: 0.34642\n",
      "--------------------------------------------------\n",
      "svc [FOLD-4 SEED-1824] Recall score: 0.66667\n",
      "svc [FOLD-4 SEED-1824] Precision score: 0.93333\n",
      "svc [FOLD-4 SEED-1824] BalancedLogLoss score: 0.38769\n",
      "--------------------------------------------------\n",
      "brf [FOLD-4 SEED-1824] Recall score: 0.85714\n",
      "brf [FOLD-4 SEED-1824] Precision score: 0.69231\n",
      "brf [FOLD-4 SEED-1824] BalancedLogLoss score: 0.37717\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-4 SEED-1824] BalancedLogLoss score 0.25062\n",
      "==================================================\n",
      "xgb01 [FOLD-0 SEED-409] Recall score: 0.90909\n",
      "xgb01 [FOLD-0 SEED-409] Precision score: 0.62500\n",
      "xgb01 [FOLD-0 SEED-409] BalancedLogLoss score: 0.24763\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-0 SEED-409] Recall score: 0.86364\n",
      "xgb02 [FOLD-0 SEED-409] Precision score: 0.63333\n",
      "xgb02 [FOLD-0 SEED-409] BalancedLogLoss score: 0.27206\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-0 SEED-409] Recall score: 0.95455\n",
      "xgb1 [FOLD-0 SEED-409] Precision score: 0.60000\n",
      "xgb1 [FOLD-0 SEED-409] BalancedLogLoss score: 0.22915\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-0 SEED-409] Recall score: 0.90909\n",
      "xgb2 [FOLD-0 SEED-409] Precision score: 0.66667\n",
      "xgb2 [FOLD-0 SEED-409] BalancedLogLoss score: 0.21937\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-0 SEED-409] Recall score: 0.81818\n",
      "xgb3 [FOLD-0 SEED-409] Precision score: 0.62069\n",
      "xgb3 [FOLD-0 SEED-409] BalancedLogLoss score: 0.31383\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-0 SEED-409] Recall score: 0.63636\n",
      "xgb5 [FOLD-0 SEED-409] Precision score: 0.70000\n",
      "xgb5 [FOLD-0 SEED-409] BalancedLogLoss score: 0.32021\n",
      "--------------------------------------------------\n",
      "svc [FOLD-0 SEED-409] Recall score: 0.81818\n",
      "svc [FOLD-0 SEED-409] Precision score: 0.81818\n",
      "svc [FOLD-0 SEED-409] BalancedLogLoss score: 0.28225\n",
      "--------------------------------------------------\n",
      "brf [FOLD-0 SEED-409] Recall score: 0.86364\n",
      "brf [FOLD-0 SEED-409] Precision score: 0.55882\n",
      "brf [FOLD-0 SEED-409] BalancedLogLoss score: 0.42110\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-0 SEED-409] BalancedLogLoss score 0.18856\n",
      "==================================================\n",
      "xgb01 [FOLD-1 SEED-409] Recall score: 0.86364\n",
      "xgb01 [FOLD-1 SEED-409] Precision score: 0.65517\n",
      "xgb01 [FOLD-1 SEED-409] BalancedLogLoss score: 0.27676\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-1 SEED-409] Recall score: 0.81818\n",
      "xgb02 [FOLD-1 SEED-409] Precision score: 0.69231\n",
      "xgb02 [FOLD-1 SEED-409] BalancedLogLoss score: 0.26778\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-1 SEED-409] Recall score: 0.81818\n",
      "xgb1 [FOLD-1 SEED-409] Precision score: 0.52941\n",
      "xgb1 [FOLD-1 SEED-409] BalancedLogLoss score: 0.44002\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-1 SEED-409] Recall score: 0.81818\n",
      "xgb2 [FOLD-1 SEED-409] Precision score: 0.75000\n",
      "xgb2 [FOLD-1 SEED-409] BalancedLogLoss score: 0.24996\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-1 SEED-409] Recall score: 0.81818\n",
      "xgb3 [FOLD-1 SEED-409] Precision score: 0.78261\n",
      "xgb3 [FOLD-1 SEED-409] BalancedLogLoss score: 0.23874\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-1 SEED-409] Recall score: 0.81818\n",
      "xgb5 [FOLD-1 SEED-409] Precision score: 0.81818\n",
      "xgb5 [FOLD-1 SEED-409] BalancedLogLoss score: 0.32384\n",
      "--------------------------------------------------\n",
      "svc [FOLD-1 SEED-409] Recall score: 0.77273\n",
      "svc [FOLD-1 SEED-409] Precision score: 0.77273\n",
      "svc [FOLD-1 SEED-409] BalancedLogLoss score: 0.31484\n",
      "--------------------------------------------------\n",
      "brf [FOLD-1 SEED-409] Recall score: 0.86364\n",
      "brf [FOLD-1 SEED-409] Precision score: 0.57576\n",
      "brf [FOLD-1 SEED-409] BalancedLogLoss score: 0.40028\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-1 SEED-409] BalancedLogLoss score 0.23524\n",
      "==================================================\n",
      "xgb01 [FOLD-2 SEED-409] Recall score: 0.86364\n",
      "xgb01 [FOLD-2 SEED-409] Precision score: 0.82609\n",
      "xgb01 [FOLD-2 SEED-409] BalancedLogLoss score: 0.29043\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-2 SEED-409] Recall score: 0.86364\n",
      "xgb02 [FOLD-2 SEED-409] Precision score: 0.70370\n",
      "xgb02 [FOLD-2 SEED-409] BalancedLogLoss score: 0.27870\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-2 SEED-409] Recall score: 0.77273\n",
      "xgb1 [FOLD-2 SEED-409] Precision score: 0.62963\n",
      "xgb1 [FOLD-2 SEED-409] BalancedLogLoss score: 0.35908\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-2 SEED-409] Recall score: 0.90909\n",
      "xgb2 [FOLD-2 SEED-409] Precision score: 0.64516\n",
      "xgb2 [FOLD-2 SEED-409] BalancedLogLoss score: 0.29032\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-2 SEED-409] Recall score: 0.86364\n",
      "xgb3 [FOLD-2 SEED-409] Precision score: 0.86364\n",
      "xgb3 [FOLD-2 SEED-409] BalancedLogLoss score: 0.31431\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-2 SEED-409] Recall score: 0.81818\n",
      "xgb5 [FOLD-2 SEED-409] Precision score: 0.85714\n",
      "xgb5 [FOLD-2 SEED-409] BalancedLogLoss score: 0.38312\n",
      "--------------------------------------------------\n",
      "svc [FOLD-2 SEED-409] Recall score: 0.59091\n",
      "svc [FOLD-2 SEED-409] Precision score: 0.81250\n",
      "svc [FOLD-2 SEED-409] BalancedLogLoss score: 0.48392\n",
      "--------------------------------------------------\n",
      "brf [FOLD-2 SEED-409] Recall score: 0.86364\n",
      "brf [FOLD-2 SEED-409] Precision score: 0.70370\n",
      "brf [FOLD-2 SEED-409] BalancedLogLoss score: 0.38575\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-2 SEED-409] BalancedLogLoss score 0.27669\n",
      "==================================================\n",
      "xgb01 [FOLD-3 SEED-409] Recall score: 0.85714\n",
      "xgb01 [FOLD-3 SEED-409] Precision score: 0.81818\n",
      "xgb01 [FOLD-3 SEED-409] BalancedLogLoss score: 0.26203\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-3 SEED-409] Recall score: 0.90476\n",
      "xgb02 [FOLD-3 SEED-409] Precision score: 0.90476\n",
      "xgb02 [FOLD-3 SEED-409] BalancedLogLoss score: 0.26291\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-3 SEED-409] Recall score: 0.90476\n",
      "xgb1 [FOLD-3 SEED-409] Precision score: 0.63333\n",
      "xgb1 [FOLD-3 SEED-409] BalancedLogLoss score: 0.37375\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-3 SEED-409] Recall score: 0.90476\n",
      "xgb2 [FOLD-3 SEED-409] Precision score: 0.76000\n",
      "xgb2 [FOLD-3 SEED-409] BalancedLogLoss score: 0.26744\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-3 SEED-409] Recall score: 0.80952\n",
      "xgb3 [FOLD-3 SEED-409] Precision score: 0.89474\n",
      "xgb3 [FOLD-3 SEED-409] BalancedLogLoss score: 0.31652\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-3 SEED-409] Recall score: 0.80952\n",
      "xgb5 [FOLD-3 SEED-409] Precision score: 0.89474\n",
      "xgb5 [FOLD-3 SEED-409] BalancedLogLoss score: 0.35219\n",
      "--------------------------------------------------\n",
      "svc [FOLD-3 SEED-409] Recall score: 0.80952\n",
      "svc [FOLD-3 SEED-409] Precision score: 0.89474\n",
      "svc [FOLD-3 SEED-409] BalancedLogLoss score: 0.39917\n",
      "--------------------------------------------------\n",
      "brf [FOLD-3 SEED-409] Recall score: 0.95238\n",
      "brf [FOLD-3 SEED-409] Precision score: 0.83333\n",
      "brf [FOLD-3 SEED-409] BalancedLogLoss score: 0.37392\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-3 SEED-409] BalancedLogLoss score 0.25168\n",
      "==================================================\n",
      "xgb01 [FOLD-4 SEED-409] Recall score: 0.90476\n",
      "xgb01 [FOLD-4 SEED-409] Precision score: 0.76000\n",
      "xgb01 [FOLD-4 SEED-409] BalancedLogLoss score: 0.18514\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-4 SEED-409] Recall score: 0.90476\n",
      "xgb02 [FOLD-4 SEED-409] Precision score: 0.70370\n",
      "xgb02 [FOLD-4 SEED-409] BalancedLogLoss score: 0.20237\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-4 SEED-409] Recall score: 0.90476\n",
      "xgb1 [FOLD-4 SEED-409] Precision score: 0.63333\n",
      "xgb1 [FOLD-4 SEED-409] BalancedLogLoss score: 0.22830\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-4 SEED-409] Recall score: 0.80952\n",
      "xgb2 [FOLD-4 SEED-409] Precision score: 0.80952\n",
      "xgb2 [FOLD-4 SEED-409] BalancedLogLoss score: 0.25242\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-4 SEED-409] Recall score: 0.85714\n",
      "xgb3 [FOLD-4 SEED-409] Precision score: 0.81818\n",
      "xgb3 [FOLD-4 SEED-409] BalancedLogLoss score: 0.22027\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-4 SEED-409] Recall score: 0.85714\n",
      "xgb5 [FOLD-4 SEED-409] Precision score: 0.78261\n",
      "xgb5 [FOLD-4 SEED-409] BalancedLogLoss score: 0.24729\n",
      "--------------------------------------------------\n",
      "svc [FOLD-4 SEED-409] Recall score: 0.66667\n",
      "svc [FOLD-4 SEED-409] Precision score: 0.77778\n",
      "svc [FOLD-4 SEED-409] BalancedLogLoss score: 0.33984\n",
      "--------------------------------------------------\n",
      "brf [FOLD-4 SEED-409] Recall score: 0.90476\n",
      "brf [FOLD-4 SEED-409] Precision score: 0.61290\n",
      "brf [FOLD-4 SEED-409] BalancedLogLoss score: 0.38188\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-4 SEED-409] BalancedLogLoss score 0.18463\n",
      "==================================================\n",
      "xgb01 [FOLD-0 SEED-4506] Recall score: 0.86364\n",
      "xgb01 [FOLD-0 SEED-4506] Precision score: 0.70370\n",
      "xgb01 [FOLD-0 SEED-4506] BalancedLogLoss score: 0.31357\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-0 SEED-4506] Recall score: 0.86364\n",
      "xgb02 [FOLD-0 SEED-4506] Precision score: 0.70370\n",
      "xgb02 [FOLD-0 SEED-4506] BalancedLogLoss score: 0.31125\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-0 SEED-4506] Recall score: 0.77273\n",
      "xgb1 [FOLD-0 SEED-4506] Precision score: 0.68000\n",
      "xgb1 [FOLD-0 SEED-4506] BalancedLogLoss score: 0.40838\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-0 SEED-4506] Recall score: 0.86364\n",
      "xgb2 [FOLD-0 SEED-4506] Precision score: 0.67857\n",
      "xgb2 [FOLD-0 SEED-4506] BalancedLogLoss score: 0.36803\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-0 SEED-4506] Recall score: 0.77273\n",
      "xgb3 [FOLD-0 SEED-4506] Precision score: 0.73913\n",
      "xgb3 [FOLD-0 SEED-4506] BalancedLogLoss score: 0.35408\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-0 SEED-4506] Recall score: 0.77273\n",
      "xgb5 [FOLD-0 SEED-4506] Precision score: 0.70833\n",
      "xgb5 [FOLD-0 SEED-4506] BalancedLogLoss score: 0.36452\n",
      "--------------------------------------------------\n",
      "svc [FOLD-0 SEED-4506] Recall score: 0.72727\n",
      "svc [FOLD-0 SEED-4506] Precision score: 0.76190\n",
      "svc [FOLD-0 SEED-4506] BalancedLogLoss score: 0.38802\n",
      "--------------------------------------------------\n",
      "brf [FOLD-0 SEED-4506] Recall score: 0.81818\n",
      "brf [FOLD-0 SEED-4506] Precision score: 0.64286\n",
      "brf [FOLD-0 SEED-4506] BalancedLogLoss score: 0.40719\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-0 SEED-4506] BalancedLogLoss score 0.30276\n",
      "==================================================\n",
      "xgb01 [FOLD-1 SEED-4506] Recall score: 0.95455\n",
      "xgb01 [FOLD-1 SEED-4506] Precision score: 0.70000\n",
      "xgb01 [FOLD-1 SEED-4506] BalancedLogLoss score: 0.19900\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-1 SEED-4506] Recall score: 0.95455\n",
      "xgb02 [FOLD-1 SEED-4506] Precision score: 0.61765\n",
      "xgb02 [FOLD-1 SEED-4506] BalancedLogLoss score: 0.20230\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-1 SEED-4506] Recall score: 0.95455\n",
      "xgb1 [FOLD-1 SEED-4506] Precision score: 0.61765\n",
      "xgb1 [FOLD-1 SEED-4506] BalancedLogLoss score: 0.22260\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-1 SEED-4506] Recall score: 0.95455\n",
      "xgb2 [FOLD-1 SEED-4506] Precision score: 0.67742\n",
      "xgb2 [FOLD-1 SEED-4506] BalancedLogLoss score: 0.21203\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-1 SEED-4506] Recall score: 0.90909\n",
      "xgb3 [FOLD-1 SEED-4506] Precision score: 0.74074\n",
      "xgb3 [FOLD-1 SEED-4506] BalancedLogLoss score: 0.18522\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-1 SEED-4506] Recall score: 0.86364\n",
      "xgb5 [FOLD-1 SEED-4506] Precision score: 0.73077\n",
      "xgb5 [FOLD-1 SEED-4506] BalancedLogLoss score: 0.24613\n",
      "--------------------------------------------------\n",
      "svc [FOLD-1 SEED-4506] Recall score: 0.68182\n",
      "svc [FOLD-1 SEED-4506] Precision score: 0.88235\n",
      "svc [FOLD-1 SEED-4506] BalancedLogLoss score: 0.30770\n",
      "--------------------------------------------------\n",
      "brf [FOLD-1 SEED-4506] Recall score: 0.95455\n",
      "brf [FOLD-1 SEED-4506] Precision score: 0.60000\n",
      "brf [FOLD-1 SEED-4506] BalancedLogLoss score: 0.34587\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-1 SEED-4506] BalancedLogLoss score 0.17932\n",
      "==================================================\n",
      "xgb01 [FOLD-2 SEED-4506] Recall score: 0.90909\n",
      "xgb01 [FOLD-2 SEED-4506] Precision score: 0.80000\n",
      "xgb01 [FOLD-2 SEED-4506] BalancedLogLoss score: 0.21954\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-2 SEED-4506] Recall score: 0.90909\n",
      "xgb02 [FOLD-2 SEED-4506] Precision score: 0.76923\n",
      "xgb02 [FOLD-2 SEED-4506] BalancedLogLoss score: 0.21405\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-2 SEED-4506] Recall score: 0.86364\n",
      "xgb1 [FOLD-2 SEED-4506] Precision score: 0.73077\n",
      "xgb1 [FOLD-2 SEED-4506] BalancedLogLoss score: 0.30471\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-2 SEED-4506] Recall score: 0.81818\n",
      "xgb2 [FOLD-2 SEED-4506] Precision score: 0.78261\n",
      "xgb2 [FOLD-2 SEED-4506] BalancedLogLoss score: 0.26096\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-2 SEED-4506] Recall score: 0.77273\n",
      "xgb3 [FOLD-2 SEED-4506] Precision score: 0.80952\n",
      "xgb3 [FOLD-2 SEED-4506] BalancedLogLoss score: 0.24816\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-2 SEED-4506] Recall score: 0.72727\n",
      "xgb5 [FOLD-2 SEED-4506] Precision score: 0.88889\n",
      "xgb5 [FOLD-2 SEED-4506] BalancedLogLoss score: 0.29378\n",
      "--------------------------------------------------\n",
      "svc [FOLD-2 SEED-4506] Recall score: 0.59091\n",
      "svc [FOLD-2 SEED-4506] Precision score: 0.81250\n",
      "svc [FOLD-2 SEED-4506] BalancedLogLoss score: 0.38266\n",
      "--------------------------------------------------\n",
      "brf [FOLD-2 SEED-4506] Recall score: 0.90909\n",
      "brf [FOLD-2 SEED-4506] Precision score: 0.68966\n",
      "brf [FOLD-2 SEED-4506] BalancedLogLoss score: 0.37094\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-2 SEED-4506] BalancedLogLoss score 0.21075\n",
      "==================================================\n",
      "xgb01 [FOLD-3 SEED-4506] Recall score: 0.95238\n",
      "xgb01 [FOLD-3 SEED-4506] Precision score: 0.76923\n",
      "xgb01 [FOLD-3 SEED-4506] BalancedLogLoss score: 0.13681\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-3 SEED-4506] Recall score: 1.00000\n",
      "xgb02 [FOLD-3 SEED-4506] Precision score: 0.75000\n",
      "xgb02 [FOLD-3 SEED-4506] BalancedLogLoss score: 0.15758\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-3 SEED-4506] Recall score: 0.85714\n",
      "xgb1 [FOLD-3 SEED-4506] Precision score: 0.60000\n",
      "xgb1 [FOLD-3 SEED-4506] BalancedLogLoss score: 0.34239\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-3 SEED-4506] Recall score: 0.90476\n",
      "xgb2 [FOLD-3 SEED-4506] Precision score: 0.79167\n",
      "xgb2 [FOLD-3 SEED-4506] BalancedLogLoss score: 0.16111\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-3 SEED-4506] Recall score: 0.95238\n",
      "xgb3 [FOLD-3 SEED-4506] Precision score: 0.76923\n",
      "xgb3 [FOLD-3 SEED-4506] BalancedLogLoss score: 0.16105\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-3 SEED-4506] Recall score: 0.85714\n",
      "xgb5 [FOLD-3 SEED-4506] Precision score: 0.85714\n",
      "xgb5 [FOLD-3 SEED-4506] BalancedLogLoss score: 0.17514\n",
      "--------------------------------------------------\n",
      "svc [FOLD-3 SEED-4506] Recall score: 0.85714\n",
      "svc [FOLD-3 SEED-4506] Precision score: 0.69231\n",
      "svc [FOLD-3 SEED-4506] BalancedLogLoss score: 0.23691\n",
      "--------------------------------------------------\n",
      "brf [FOLD-3 SEED-4506] Recall score: 0.95238\n",
      "brf [FOLD-3 SEED-4506] Precision score: 0.64516\n",
      "brf [FOLD-3 SEED-4506] BalancedLogLoss score: 0.35232\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-3 SEED-4506] BalancedLogLoss score 0.13996\n",
      "==================================================\n",
      "xgb01 [FOLD-4 SEED-4506] Recall score: 0.85714\n",
      "xgb01 [FOLD-4 SEED-4506] Precision score: 0.81818\n",
      "xgb01 [FOLD-4 SEED-4506] BalancedLogLoss score: 0.33858\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-4 SEED-4506] Recall score: 0.85714\n",
      "xgb02 [FOLD-4 SEED-4506] Precision score: 0.75000\n",
      "xgb02 [FOLD-4 SEED-4506] BalancedLogLoss score: 0.29063\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-4 SEED-4506] Recall score: 0.85714\n",
      "xgb1 [FOLD-4 SEED-4506] Precision score: 0.62069\n",
      "xgb1 [FOLD-4 SEED-4506] BalancedLogLoss score: 0.33425\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-4 SEED-4506] Recall score: 0.76190\n",
      "xgb2 [FOLD-4 SEED-4506] Precision score: 0.66667\n",
      "xgb2 [FOLD-4 SEED-4506] BalancedLogLoss score: 0.32450\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-4 SEED-4506] Recall score: 0.80952\n",
      "xgb3 [FOLD-4 SEED-4506] Precision score: 0.80952\n",
      "xgb3 [FOLD-4 SEED-4506] BalancedLogLoss score: 0.37813\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-4 SEED-4506] Recall score: 0.76190\n",
      "xgb5 [FOLD-4 SEED-4506] Precision score: 0.94118\n",
      "xgb5 [FOLD-4 SEED-4506] BalancedLogLoss score: 0.36499\n",
      "--------------------------------------------------\n",
      "svc [FOLD-4 SEED-4506] Recall score: 0.66667\n",
      "svc [FOLD-4 SEED-4506] Precision score: 0.93333\n",
      "svc [FOLD-4 SEED-4506] BalancedLogLoss score: 0.52067\n",
      "--------------------------------------------------\n",
      "brf [FOLD-4 SEED-4506] Recall score: 0.85714\n",
      "brf [FOLD-4 SEED-4506] Precision score: 0.78261\n",
      "brf [FOLD-4 SEED-4506] BalancedLogLoss score: 0.38880\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-4 SEED-4506] BalancedLogLoss score 0.29015\n",
      "==================================================\n",
      "xgb01 [FOLD-0 SEED-4012] Recall score: 0.86364\n",
      "xgb01 [FOLD-0 SEED-4012] Precision score: 0.73077\n",
      "xgb01 [FOLD-0 SEED-4012] BalancedLogLoss score: 0.23773\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-0 SEED-4012] Recall score: 0.90909\n",
      "xgb02 [FOLD-0 SEED-4012] Precision score: 0.71429\n",
      "xgb02 [FOLD-0 SEED-4012] BalancedLogLoss score: 0.24380\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-0 SEED-4012] Recall score: 0.90909\n",
      "xgb1 [FOLD-0 SEED-4012] Precision score: 0.60606\n",
      "xgb1 [FOLD-0 SEED-4012] BalancedLogLoss score: 0.29507\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-0 SEED-4012] Recall score: 0.81818\n",
      "xgb2 [FOLD-0 SEED-4012] Precision score: 0.69231\n",
      "xgb2 [FOLD-0 SEED-4012] BalancedLogLoss score: 0.27468\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-0 SEED-4012] Recall score: 0.81818\n",
      "xgb3 [FOLD-0 SEED-4012] Precision score: 0.81818\n",
      "xgb3 [FOLD-0 SEED-4012] BalancedLogLoss score: 0.22384\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-0 SEED-4012] Recall score: 0.77273\n",
      "xgb5 [FOLD-0 SEED-4012] Precision score: 0.85000\n",
      "xgb5 [FOLD-0 SEED-4012] BalancedLogLoss score: 0.28649\n",
      "--------------------------------------------------\n",
      "svc [FOLD-0 SEED-4012] Recall score: 0.72727\n",
      "svc [FOLD-0 SEED-4012] Precision score: 1.00000\n",
      "svc [FOLD-0 SEED-4012] BalancedLogLoss score: 0.39231\n",
      "--------------------------------------------------\n",
      "brf [FOLD-0 SEED-4012] Recall score: 0.81818\n",
      "brf [FOLD-0 SEED-4012] Precision score: 0.72000\n",
      "brf [FOLD-0 SEED-4012] BalancedLogLoss score: 0.37714\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-0 SEED-4012] BalancedLogLoss score 0.22316\n",
      "==================================================\n",
      "xgb01 [FOLD-1 SEED-4012] Recall score: 0.86364\n",
      "xgb01 [FOLD-1 SEED-4012] Precision score: 0.61290\n",
      "xgb01 [FOLD-1 SEED-4012] BalancedLogLoss score: 0.36626\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-1 SEED-4012] Recall score: 0.81818\n",
      "xgb02 [FOLD-1 SEED-4012] Precision score: 0.64286\n",
      "xgb02 [FOLD-1 SEED-4012] BalancedLogLoss score: 0.35844\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-1 SEED-4012] Recall score: 0.81818\n",
      "xgb1 [FOLD-1 SEED-4012] Precision score: 0.54545\n",
      "xgb1 [FOLD-1 SEED-4012] BalancedLogLoss score: 0.47361\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-1 SEED-4012] Recall score: 0.77273\n",
      "xgb2 [FOLD-1 SEED-4012] Precision score: 0.65385\n",
      "xgb2 [FOLD-1 SEED-4012] BalancedLogLoss score: 0.41473\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-1 SEED-4012] Recall score: 0.72727\n",
      "xgb3 [FOLD-1 SEED-4012] Precision score: 0.72727\n",
      "xgb3 [FOLD-1 SEED-4012] BalancedLogLoss score: 0.40785\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-1 SEED-4012] Recall score: 0.77273\n",
      "xgb5 [FOLD-1 SEED-4012] Precision score: 0.65385\n",
      "xgb5 [FOLD-1 SEED-4012] BalancedLogLoss score: 0.40845\n",
      "--------------------------------------------------\n",
      "svc [FOLD-1 SEED-4012] Recall score: 0.72727\n",
      "svc [FOLD-1 SEED-4012] Precision score: 0.80000\n",
      "svc [FOLD-1 SEED-4012] BalancedLogLoss score: 0.49075\n",
      "--------------------------------------------------\n",
      "brf [FOLD-1 SEED-4012] Recall score: 0.86364\n",
      "brf [FOLD-1 SEED-4012] Precision score: 0.57576\n",
      "brf [FOLD-1 SEED-4012] BalancedLogLoss score: 0.41419\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-1 SEED-4012] BalancedLogLoss score 0.33182\n",
      "==================================================\n",
      "xgb01 [FOLD-2 SEED-4012] Recall score: 0.90909\n",
      "xgb01 [FOLD-2 SEED-4012] Precision score: 0.86957\n",
      "xgb01 [FOLD-2 SEED-4012] BalancedLogLoss score: 0.21475\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-2 SEED-4012] Recall score: 0.90909\n",
      "xgb02 [FOLD-2 SEED-4012] Precision score: 0.90909\n",
      "xgb02 [FOLD-2 SEED-4012] BalancedLogLoss score: 0.19162\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-2 SEED-4012] Recall score: 0.86364\n",
      "xgb1 [FOLD-2 SEED-4012] Precision score: 0.73077\n",
      "xgb1 [FOLD-2 SEED-4012] BalancedLogLoss score: 0.32601\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-2 SEED-4012] Recall score: 0.90909\n",
      "xgb2 [FOLD-2 SEED-4012] Precision score: 0.90909\n",
      "xgb2 [FOLD-2 SEED-4012] BalancedLogLoss score: 0.19679\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-2 SEED-4012] Recall score: 0.86364\n",
      "xgb3 [FOLD-2 SEED-4012] Precision score: 0.90476\n",
      "xgb3 [FOLD-2 SEED-4012] BalancedLogLoss score: 0.23011\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-2 SEED-4012] Recall score: 0.81818\n",
      "xgb5 [FOLD-2 SEED-4012] Precision score: 0.90000\n",
      "xgb5 [FOLD-2 SEED-4012] BalancedLogLoss score: 0.28373\n",
      "--------------------------------------------------\n",
      "svc [FOLD-2 SEED-4012] Recall score: 0.90909\n",
      "svc [FOLD-2 SEED-4012] Precision score: 0.83333\n",
      "svc [FOLD-2 SEED-4012] BalancedLogLoss score: 0.28613\n",
      "--------------------------------------------------\n",
      "brf [FOLD-2 SEED-4012] Recall score: 0.90909\n",
      "brf [FOLD-2 SEED-4012] Precision score: 0.71429\n",
      "brf [FOLD-2 SEED-4012] BalancedLogLoss score: 0.35382\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-2 SEED-4012] BalancedLogLoss score 0.18684\n",
      "==================================================\n",
      "xgb01 [FOLD-3 SEED-4012] Recall score: 0.95238\n",
      "xgb01 [FOLD-3 SEED-4012] Precision score: 0.76923\n",
      "xgb01 [FOLD-3 SEED-4012] BalancedLogLoss score: 0.18792\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-3 SEED-4012] Recall score: 0.95238\n",
      "xgb02 [FOLD-3 SEED-4012] Precision score: 0.71429\n",
      "xgb02 [FOLD-3 SEED-4012] BalancedLogLoss score: 0.18926\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-3 SEED-4012] Recall score: 0.85714\n",
      "xgb1 [FOLD-3 SEED-4012] Precision score: 0.54545\n",
      "xgb1 [FOLD-3 SEED-4012] BalancedLogLoss score: 0.35533\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-3 SEED-4012] Recall score: 0.90476\n",
      "xgb2 [FOLD-3 SEED-4012] Precision score: 0.70370\n",
      "xgb2 [FOLD-3 SEED-4012] BalancedLogLoss score: 0.24834\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-3 SEED-4012] Recall score: 0.80952\n",
      "xgb3 [FOLD-3 SEED-4012] Precision score: 0.70833\n",
      "xgb3 [FOLD-3 SEED-4012] BalancedLogLoss score: 0.21938\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-3 SEED-4012] Recall score: 0.80952\n",
      "xgb5 [FOLD-3 SEED-4012] Precision score: 0.77273\n",
      "xgb5 [FOLD-3 SEED-4012] BalancedLogLoss score: 0.23720\n",
      "--------------------------------------------------\n",
      "svc [FOLD-3 SEED-4012] Recall score: 0.61905\n",
      "svc [FOLD-3 SEED-4012] Precision score: 0.72222\n",
      "svc [FOLD-3 SEED-4012] BalancedLogLoss score: 0.38301\n",
      "--------------------------------------------------\n",
      "brf [FOLD-3 SEED-4012] Recall score: 0.95238\n",
      "brf [FOLD-3 SEED-4012] Precision score: 0.64516\n",
      "brf [FOLD-3 SEED-4012] BalancedLogLoss score: 0.36547\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-3 SEED-4012] BalancedLogLoss score 0.18197\n",
      "==================================================\n",
      "xgb01 [FOLD-4 SEED-4012] Recall score: 0.85714\n",
      "xgb01 [FOLD-4 SEED-4012] Precision score: 0.72000\n",
      "xgb01 [FOLD-4 SEED-4012] BalancedLogLoss score: 0.21269\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-4 SEED-4012] Recall score: 0.76190\n",
      "xgb02 [FOLD-4 SEED-4012] Precision score: 0.72727\n",
      "xgb02 [FOLD-4 SEED-4012] BalancedLogLoss score: 0.23943\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-4 SEED-4012] Recall score: 1.00000\n",
      "xgb1 [FOLD-4 SEED-4012] Precision score: 0.58333\n",
      "xgb1 [FOLD-4 SEED-4012] BalancedLogLoss score: 0.26875\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-4 SEED-4012] Recall score: 0.76190\n",
      "xgb2 [FOLD-4 SEED-4012] Precision score: 0.69565\n",
      "xgb2 [FOLD-4 SEED-4012] BalancedLogLoss score: 0.26659\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-4 SEED-4012] Recall score: 0.76190\n",
      "xgb3 [FOLD-4 SEED-4012] Precision score: 0.72727\n",
      "xgb3 [FOLD-4 SEED-4012] BalancedLogLoss score: 0.26279\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-4 SEED-4012] Recall score: 0.80952\n",
      "xgb5 [FOLD-4 SEED-4012] Precision score: 0.77273\n",
      "xgb5 [FOLD-4 SEED-4012] BalancedLogLoss score: 0.26518\n",
      "--------------------------------------------------\n",
      "svc [FOLD-4 SEED-4012] Recall score: 0.80952\n",
      "svc [FOLD-4 SEED-4012] Precision score: 0.89474\n",
      "svc [FOLD-4 SEED-4012] BalancedLogLoss score: 0.28936\n",
      "--------------------------------------------------\n",
      "brf [FOLD-4 SEED-4012] Recall score: 0.95238\n",
      "brf [FOLD-4 SEED-4012] Precision score: 0.68966\n",
      "brf [FOLD-4 SEED-4012] BalancedLogLoss score: 0.37799\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-4 SEED-4012] BalancedLogLoss score 0.19236\n",
      "==================================================\n",
      "xgb01 [FOLD-0 SEED-3657] Recall score: 0.77273\n",
      "xgb01 [FOLD-0 SEED-3657] Precision score: 0.73913\n",
      "xgb01 [FOLD-0 SEED-3657] BalancedLogLoss score: 0.38944\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-0 SEED-3657] Recall score: 0.72727\n",
      "xgb02 [FOLD-0 SEED-3657] Precision score: 0.69565\n",
      "xgb02 [FOLD-0 SEED-3657] BalancedLogLoss score: 0.40526\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-0 SEED-3657] Recall score: 0.72727\n",
      "xgb1 [FOLD-0 SEED-3657] Precision score: 0.66667\n",
      "xgb1 [FOLD-0 SEED-3657] BalancedLogLoss score: 0.47521\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-0 SEED-3657] Recall score: 0.77273\n",
      "xgb2 [FOLD-0 SEED-3657] Precision score: 0.62963\n",
      "xgb2 [FOLD-0 SEED-3657] BalancedLogLoss score: 0.40156\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-0 SEED-3657] Recall score: 0.63636\n",
      "xgb3 [FOLD-0 SEED-3657] Precision score: 0.82353\n",
      "xgb3 [FOLD-0 SEED-3657] BalancedLogLoss score: 0.48889\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-0 SEED-3657] Recall score: 0.68182\n",
      "xgb5 [FOLD-0 SEED-3657] Precision score: 0.78947\n",
      "xgb5 [FOLD-0 SEED-3657] BalancedLogLoss score: 0.49400\n",
      "--------------------------------------------------\n",
      "svc [FOLD-0 SEED-3657] Recall score: 0.68182\n",
      "svc [FOLD-0 SEED-3657] Precision score: 0.93750\n",
      "svc [FOLD-0 SEED-3657] BalancedLogLoss score: 0.71785\n",
      "--------------------------------------------------\n",
      "brf [FOLD-0 SEED-3657] Recall score: 0.77273\n",
      "brf [FOLD-0 SEED-3657] Precision score: 0.68000\n",
      "brf [FOLD-0 SEED-3657] BalancedLogLoss score: 0.45391\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-0 SEED-3657] BalancedLogLoss score 0.38381\n",
      "==================================================\n",
      "xgb01 [FOLD-1 SEED-3657] Recall score: 0.90909\n",
      "xgb01 [FOLD-1 SEED-3657] Precision score: 0.80000\n",
      "xgb01 [FOLD-1 SEED-3657] BalancedLogLoss score: 0.22753\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-1 SEED-3657] Recall score: 0.90909\n",
      "xgb02 [FOLD-1 SEED-3657] Precision score: 0.74074\n",
      "xgb02 [FOLD-1 SEED-3657] BalancedLogLoss score: 0.22506\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-1 SEED-3657] Recall score: 0.77273\n",
      "xgb1 [FOLD-1 SEED-3657] Precision score: 0.48571\n",
      "xgb1 [FOLD-1 SEED-3657] BalancedLogLoss score: 0.44866\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-1 SEED-3657] Recall score: 0.86364\n",
      "xgb2 [FOLD-1 SEED-3657] Precision score: 0.70370\n",
      "xgb2 [FOLD-1 SEED-3657] BalancedLogLoss score: 0.26431\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-1 SEED-3657] Recall score: 0.90909\n",
      "xgb3 [FOLD-1 SEED-3657] Precision score: 0.80000\n",
      "xgb3 [FOLD-1 SEED-3657] BalancedLogLoss score: 0.22485\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-1 SEED-3657] Recall score: 0.77273\n",
      "xgb5 [FOLD-1 SEED-3657] Precision score: 0.77273\n",
      "xgb5 [FOLD-1 SEED-3657] BalancedLogLoss score: 0.29026\n",
      "--------------------------------------------------\n",
      "svc [FOLD-1 SEED-3657] Recall score: 0.63636\n",
      "svc [FOLD-1 SEED-3657] Precision score: 0.77778\n",
      "svc [FOLD-1 SEED-3657] BalancedLogLoss score: 0.31236\n",
      "--------------------------------------------------\n",
      "brf [FOLD-1 SEED-3657] Recall score: 0.95455\n",
      "brf [FOLD-1 SEED-3657] Precision score: 0.67742\n",
      "brf [FOLD-1 SEED-3657] BalancedLogLoss score: 0.37388\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-1 SEED-3657] BalancedLogLoss score 0.21276\n",
      "==================================================\n",
      "xgb01 [FOLD-2 SEED-3657] Recall score: 0.95455\n",
      "xgb01 [FOLD-2 SEED-3657] Precision score: 0.70000\n",
      "xgb01 [FOLD-2 SEED-3657] BalancedLogLoss score: 0.18406\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-2 SEED-3657] Recall score: 0.95455\n",
      "xgb02 [FOLD-2 SEED-3657] Precision score: 0.61765\n",
      "xgb02 [FOLD-2 SEED-3657] BalancedLogLoss score: 0.21609\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-2 SEED-3657] Recall score: 0.86364\n",
      "xgb1 [FOLD-2 SEED-3657] Precision score: 0.59375\n",
      "xgb1 [FOLD-2 SEED-3657] BalancedLogLoss score: 0.35096\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-2 SEED-3657] Recall score: 0.86364\n",
      "xgb2 [FOLD-2 SEED-3657] Precision score: 0.61290\n",
      "xgb2 [FOLD-2 SEED-3657] BalancedLogLoss score: 0.25764\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-2 SEED-3657] Recall score: 0.86364\n",
      "xgb3 [FOLD-2 SEED-3657] Precision score: 0.67857\n",
      "xgb3 [FOLD-2 SEED-3657] BalancedLogLoss score: 0.21432\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-2 SEED-3657] Recall score: 0.86364\n",
      "xgb5 [FOLD-2 SEED-3657] Precision score: 0.73077\n",
      "xgb5 [FOLD-2 SEED-3657] BalancedLogLoss score: 0.23638\n",
      "--------------------------------------------------\n",
      "svc [FOLD-2 SEED-3657] Recall score: 0.68182\n",
      "svc [FOLD-2 SEED-3657] Precision score: 0.71429\n",
      "svc [FOLD-2 SEED-3657] BalancedLogLoss score: 0.35523\n",
      "--------------------------------------------------\n",
      "brf [FOLD-2 SEED-3657] Recall score: 0.95455\n",
      "brf [FOLD-2 SEED-3657] Precision score: 0.60000\n",
      "brf [FOLD-2 SEED-3657] BalancedLogLoss score: 0.37748\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-2 SEED-3657] BalancedLogLoss score 0.18553\n",
      "==================================================\n",
      "xgb01 [FOLD-3 SEED-3657] Recall score: 0.95238\n",
      "xgb01 [FOLD-3 SEED-3657] Precision score: 0.80000\n",
      "xgb01 [FOLD-3 SEED-3657] BalancedLogLoss score: 0.16333\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-3 SEED-3657] Recall score: 0.95238\n",
      "xgb02 [FOLD-3 SEED-3657] Precision score: 0.86957\n",
      "xgb02 [FOLD-3 SEED-3657] BalancedLogLoss score: 0.17269\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-3 SEED-3657] Recall score: 0.95238\n",
      "xgb1 [FOLD-3 SEED-3657] Precision score: 0.71429\n",
      "xgb1 [FOLD-3 SEED-3657] BalancedLogLoss score: 0.24732\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-3 SEED-3657] Recall score: 0.95238\n",
      "xgb2 [FOLD-3 SEED-3657] Precision score: 0.86957\n",
      "xgb2 [FOLD-3 SEED-3657] BalancedLogLoss score: 0.14530\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-3 SEED-3657] Recall score: 0.95238\n",
      "xgb3 [FOLD-3 SEED-3657] Precision score: 0.86957\n",
      "xgb3 [FOLD-3 SEED-3657] BalancedLogLoss score: 0.15263\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-3 SEED-3657] Recall score: 0.90476\n",
      "xgb5 [FOLD-3 SEED-3657] Precision score: 0.82609\n",
      "xgb5 [FOLD-3 SEED-3657] BalancedLogLoss score: 0.22130\n",
      "--------------------------------------------------\n",
      "svc [FOLD-3 SEED-3657] Recall score: 0.85714\n",
      "svc [FOLD-3 SEED-3657] Precision score: 1.00000\n",
      "svc [FOLD-3 SEED-3657] BalancedLogLoss score: 0.20539\n",
      "--------------------------------------------------\n",
      "brf [FOLD-3 SEED-3657] Recall score: 0.95238\n",
      "brf [FOLD-3 SEED-3657] Precision score: 0.71429\n",
      "brf [FOLD-3 SEED-3657] BalancedLogLoss score: 0.33826\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-3 SEED-3657] BalancedLogLoss score 0.14090\n",
      "==================================================\n",
      "xgb01 [FOLD-4 SEED-3657] Recall score: 0.90476\n",
      "xgb01 [FOLD-4 SEED-3657] Precision score: 0.73077\n",
      "xgb01 [FOLD-4 SEED-3657] BalancedLogLoss score: 0.21202\n",
      "--------------------------------------------------\n",
      "xgb02 [FOLD-4 SEED-3657] Recall score: 0.95238\n",
      "xgb02 [FOLD-4 SEED-3657] Precision score: 0.80000\n",
      "xgb02 [FOLD-4 SEED-3657] BalancedLogLoss score: 0.18550\n",
      "--------------------------------------------------\n",
      "xgb1 [FOLD-4 SEED-3657] Recall score: 0.76190\n",
      "xgb1 [FOLD-4 SEED-3657] Precision score: 0.69565\n",
      "xgb1 [FOLD-4 SEED-3657] BalancedLogLoss score: 0.43243\n",
      "--------------------------------------------------\n",
      "xgb2 [FOLD-4 SEED-3657] Recall score: 0.85714\n",
      "xgb2 [FOLD-4 SEED-3657] Precision score: 0.85714\n",
      "xgb2 [FOLD-4 SEED-3657] BalancedLogLoss score: 0.16970\n",
      "--------------------------------------------------\n",
      "xgb3 [FOLD-4 SEED-3657] Recall score: 0.80952\n",
      "xgb3 [FOLD-4 SEED-3657] Precision score: 0.77273\n",
      "xgb3 [FOLD-4 SEED-3657] BalancedLogLoss score: 0.18966\n",
      "--------------------------------------------------\n",
      "xgb5 [FOLD-4 SEED-3657] Recall score: 0.76190\n",
      "xgb5 [FOLD-4 SEED-3657] Precision score: 0.84211\n",
      "xgb5 [FOLD-4 SEED-3657] BalancedLogLoss score: 0.27154\n",
      "--------------------------------------------------\n",
      "svc [FOLD-4 SEED-3657] Recall score: 0.71429\n",
      "svc [FOLD-4 SEED-3657] Precision score: 0.75000\n",
      "svc [FOLD-4 SEED-3657] BalancedLogLoss score: 0.37843\n",
      "--------------------------------------------------\n",
      "brf [FOLD-4 SEED-3657] Recall score: 0.90476\n",
      "brf [FOLD-4 SEED-3657] Precision score: 0.65517\n",
      "brf [FOLD-4 SEED-3657] BalancedLogLoss score: 0.35443\n",
      "--------------------------------------------------\n",
      "--> Ensemble [FOLD-4 SEED-3657] BalancedLogLoss score 0.17538\n",
      "==================================================\n",
      "CPU times: user 12min 46s, sys: 3min 47s, total: 16min 34s\n",
      "Wall time: 8min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "kfold = 'skf'\n",
    "n_splits = 5\n",
    "n_reapts = 5\n",
    "random_state = 42\n",
    "n_estimators = 99999\n",
    "early_stopping_rounds = 99\n",
    "verbose = False\n",
    "device = 'cpu'\n",
    "\n",
    "# Fix seed\n",
    "random.seed(random_state)\n",
    "random_state_list = random.sample(range(9999), n_reapts)\n",
    "#random_state_list = [42]\n",
    "\n",
    "# Initialize an array for storing test predictions\n",
    "classifier = Classifier(n_estimators, device, random_state)\n",
    "test_predss = np.zeros((X_test.shape[0]))\n",
    "oof_predss = np.zeros((X_train.shape[0], n_reapts))\n",
    "ensemble_score, ensemble_score_ = [], []\n",
    "weights = []\n",
    "oof_each_predss = []\n",
    "oof_each_preds = np.zeros((X_train.shape[0], classifier.len_models))\n",
    "test_each_predss = []\n",
    "test_each_preds = np.zeros((X_test.shape[0], classifier.len_models))\n",
    "trained_models = {'xgb':[], 'cat':[]}\n",
    "score_dict = dict(zip(classifier.models_name, [[] for _ in range(classifier.len_models)]))\n",
    "\n",
    "splitter = Splitter(kfold=kfold, n_splits=n_splits, greeks=greeks.iloc[:,1:-1])\n",
    "for i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n",
    "    n = i % n_splits\n",
    "    m = i // n_splits\n",
    "            \n",
    "    # Get a set of classifier models\n",
    "    classifier = Classifier(n_estimators, device, random_state_list[m])\n",
    "    models = classifier.models\n",
    "    \n",
    "    # Initialize lists to store oof and test predictions for each base model\n",
    "    oof_preds = []\n",
    "    test_preds = []\n",
    "    \n",
    "    # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n",
    "    for name, model in models.items():\n",
    "        if ('xgb' in name) or ('lgb' in name) or ('cat' in name):\n",
    "            train_w0, train_w1 = calc_log_loss_weight(y_train_)\n",
    "            valid_w0, valid_w1 = calc_log_loss_weight(y_val)\n",
    "            if 'xgb' in name:\n",
    "                model.fit(\n",
    "                    X_train_, y_train_, sample_weight=y_train_.map({0: train_w0, 1: train_w1}), \n",
    "                    eval_set=[(X_val, y_val)], sample_weight_eval_set=[y_val.map({0: valid_w0, 1: valid_w1})],\n",
    "                    early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n",
    "            elif 'lgb' in name:\n",
    "                model.fit(\n",
    "                    X_train_, y_train_, sample_weight=y_train_.map({0: train_w0, 1: train_w1}), \n",
    "                    eval_set=[(X_val, y_val)], eval_sample_weight=[y_val.map({0: valid_w0, 1: valid_w1})],\n",
    "                    early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n",
    "            elif 'cat' in name:\n",
    "                model.fit(\n",
    "                    Pool(X_train_, y_train_, weight=y_train_.map({0: train_w0, 1: train_w1})), \n",
    "                    eval_set=Pool(X_val, y_val, weight=y_val.map({0: valid_w0, 1: valid_w1})), \n",
    "                    early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n",
    "        else:\n",
    "            model.fit(X_train_, y_train_)\n",
    "            \n",
    "        if name in trained_models.keys():\n",
    "            trained_models[f'{name}'].append(deepcopy(model))\n",
    "        \n",
    "        test_pred = model.predict_proba(X_test)[:, 1].reshape(-1)\n",
    "        y_val_pred = model.predict_proba(X_val)[:, 1].reshape(-1)\n",
    "        \n",
    "        # Calculate recall and precision scores\n",
    "        y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
    "        recall = recall_score(y_val, y_val_pred_binary)\n",
    "        precision = precision_score(y_val, y_val_pred_binary)\n",
    "        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] Recall score: {recall:.5f}')\n",
    "        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] Precision score: {precision:.5f}')\n",
    "\n",
    "        score = balanced_log_loss(y_val, y_val_pred)\n",
    "        score_dict[name].append(score)\n",
    "        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] BalancedLogLoss score: {score:.5f}')\n",
    "        print('-'*50)\n",
    "        \n",
    "        oof_preds.append(y_val_pred)\n",
    "        test_preds.append(test_pred)\n",
    "    \n",
    "    # Use Optuna to find the best ensemble weights\n",
    "    optweights = OptunaWeights(random_state=random_state_list[m])\n",
    "    y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n",
    "    \n",
    "    score = balanced_log_loss(y_val, y_val_pred)\n",
    "    score_ = roc_auc_score(y_val, y_val_pred)\n",
    "    print(f'--> Ensemble [FOLD-{n} SEED-{random_state_list[m]}] BalancedLogLoss score {score:.5f}')\n",
    "    print('='*50)\n",
    "    ensemble_score.append(score)\n",
    "    ensemble_score_.append(score_)\n",
    "    weights.append(optweights.weights)\n",
    "    \n",
    "    # Predict to X_test by the best ensemble weights\n",
    "    test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n",
    "    oof_predss[X_val.index, m] += optweights.predict(oof_preds)\n",
    "    oof_each_preds[X_val.index] = np.stack(oof_preds).T\n",
    "    test_each_preds += np.array(test_preds).T / n_splits\n",
    "    if n == (n_splits - 1):\n",
    "        oof_each_predss.append(oof_each_preds)\n",
    "        oof_each_preds = np.zeros((X_train.shape[0], classifier.len_models))\n",
    "        test_each_predss.append(test_each_preds)\n",
    "        test_each_preds = np.zeros((X_test.shape[0], classifier.len_models))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "oof_each_predss = np.mean(np.array(oof_each_predss), axis=0)\n",
    "test_each_predss = np.mean(np.array(test_each_predss), axis=0)\n",
    "oof_each_predss = np.concatenate([oof_each_predss, np.mean(oof_predss, axis=1).reshape(-1, 1)], axis=1)\n",
    "test_each_predss = np.concatenate([test_each_predss, test_predss.reshape(-1, 1)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6158dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T15:11:26.974522Z",
     "iopub.status.busy": "2023-08-09T15:11:26.973981Z",
     "iopub.status.idle": "2023-08-09T15:11:35.794626Z",
     "shell.execute_reply": "2023-08-09T15:11:35.793395Z"
    },
    "papermill": {
     "duration": 8.885565,
     "end_time": "2023-08-09T15:11:35.797353",
     "exception": false,
     "start_time": "2023-08-09T15:11:26.911788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Optuna Ensemble 0.22436  0.05852 \n",
      "\n",
      "--- Optuna Weights---\n",
      "xgb01: 0.42230  0.42454\n",
      "xgb02: 0.30196  0.38023\n",
      "xgb1: 0.24492  0.32594\n",
      "xgb2: 0.33192  0.36260\n",
      "xgb3: 0.18813  0.32430\n",
      "xgb5: 0.00408  0.00369\n",
      "svc: 0.07584  0.10598\n",
      "brf: 0.02079  0.05218\n",
      "Ensemble BalancedLogLoss score 0.22436  0.05852\n",
      "--- Model Weights ---\n",
      "xgb01: 0.42230  0.42454\n",
      "xgb02: 0.30196  0.38023\n",
      "xgb1: 0.24492  0.32594\n",
      "xgb2: 0.33192  0.36260\n",
      "xgb3: 0.18813  0.32430\n",
      "xgb5: 0.00408  0.00369\n",
      "svc: 0.07584  0.10598\n",
      "brf: 0.02079  0.05218\n",
      "\n",
      "Stacking BalancedLogLoss score 0.20853  0.06418\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>0.657909</td>\n",
       "      <td>0.342091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>0.657909</td>\n",
       "      <td>0.342091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>0.657909</td>\n",
       "      <td>0.342091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>0.657909</td>\n",
       "      <td>0.342091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>0.657909</td>\n",
       "      <td>0.342091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id   class_0   class_1\n",
       "0  00eed32682bb  0.657909  0.342091\n",
       "1  010ebe33f668  0.657909  0.342091\n",
       "2  02fa521e1838  0.657909  0.342091\n",
       "3  040e15f562a2  0.657909  0.342091\n",
       "4  046e85c7cc7f  0.657909  0.342091"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean score of the ensemble\n",
    "mean_score = np.mean(ensemble_score)\n",
    "std_score = np.std(ensemble_score)\n",
    "print(f'Mean Optuna Ensemble {mean_score:.5f}  {std_score:.5f} \\n')\n",
    "\n",
    "print('--- Optuna Weights---')\n",
    "mean_weights = np.mean(weights, axis=0)\n",
    "std_weights = np.std(weights, axis=0)\n",
    "for name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n",
    "    print(f'{name}: {mean_weight:.5f}  {std_weight:.5f}')\n",
    "stack_test_predss = np.zeros((X_test.shape[0]))\n",
    "stack_scores = []\n",
    "stack_models = []\n",
    "splitter = Splitter(kfold=kfold, n_splits=n_splits, greeks=greeks.iloc[:,1:-1])\n",
    "for i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(oof_each_predss, y_train, random_state_list=random_state_list)):\n",
    "    n = i % n_splits\n",
    "    m = i // n_splits\n",
    "    \n",
    "    classifier = Classifier(n_estimators, device, random_state_list[m])\n",
    "    models = classifier.models\n",
    "    model = models['xgb02']\n",
    "    \n",
    "    train_w0, train_w1 = calc_log_loss_weight(y_train_)\n",
    "    valid_w0, valid_w1 = calc_log_loss_weight(y_val)\n",
    "    \n",
    "    model.fit(\n",
    "    X_train_, y_train_, sample_weight=y_train_.map({0: train_w0, 1: train_w1}),\n",
    "    eval_set=[(X_val, y_val)],\n",
    "   # eval_metric='logloss',\n",
    "    sample_weight_eval_set=[y_val.map({0: valid_w0, 1: valid_w1})],\n",
    "    early_stopping_rounds=early_stopping_rounds,\n",
    "    verbose=verbose\n",
    ")\n",
    "    \n",
    "    test_pred = model.predict_proba(test_each_predss)[:, 1].reshape(-1)\n",
    "    y_val_pred = model.predict_proba(X_val)[:, 1].reshape(-1)\n",
    "\n",
    "    score = balanced_log_loss(y_val, y_val_pred)\n",
    "    stack_scores.append(score)\n",
    "    stack_models.append(deepcopy(model))\n",
    "    \n",
    "    stack_test_predss += test_pred / (n_splits * len(random_state_list))\n",
    "\n",
    "# Calculate the mean LogLoss score of the ensemble\n",
    "mean_score = np.mean(ensemble_score)\n",
    "std_score = np.std(ensemble_score)\n",
    "print(f'Ensemble BalancedLogLoss score {mean_score:.5f}  {std_score:.5f}')\n",
    "# Print the mean and standard deviation of the ensemble weights for each model\n",
    "print('--- Model Weights ---')\n",
    "mean_weights = np.mean(weights, axis=0)\n",
    "std_weights = np.std(weights, axis=0)\n",
    "for name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n",
    "    print(f'{name}: {mean_weight:.5f}  {std_weight:.5f}')\n",
    "print('')\n",
    "\n",
    "# Calculate the mean LogLoss score of the ensemble\n",
    "mean_score = np.mean(stack_scores)\n",
    "std_score = np.std(stack_scores)\n",
    "print(f'Stacking BalancedLogLoss score {mean_score:.5f}  {std_score:.5f}\\n')\n",
    "\n",
    "stackedxgbs = pd.read_csv(os.path.join(filepath, 'sample_submission.csv'))\n",
    "\n",
    "stackedxgbs['class_1'] = stack_test_predss\n",
    "stackedxgbs['class_0'] = 1 - stack_test_predss\n",
    "stackedxgbs.to_csv('stackedxgbs0.15.csv', index=False)\n",
    "stackedxgbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2011d4e4",
   "metadata": {
    "papermill": {
     "duration": 0.035517,
     "end_time": "2023-08-09T15:11:35.873062",
     "exception": false,
     "start_time": "2023-08-09T15:11:35.837545",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# model 3 -> tabpfn + xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c2214a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T15:11:35.948823Z",
     "iopub.status.busy": "2023-08-09T15:11:35.947788Z",
     "iopub.status.idle": "2023-08-09T15:11:54.342378Z",
     "shell.execute_reply": "2023-08-09T15:11:54.340435Z"
    },
    "papermill": {
     "duration": 18.435428,
     "end_time": "2023-08-09T15:11:54.345357",
     "exception": false,
     "start_time": "2023-08-09T15:11:35.909929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install tabpfn --no-index --find-links=file:///kaggle/input/pip-packages-icr/pip-packages\n",
    "!mkdir -p /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n",
    "!cp /kaggle/input/pip-packages-icr/pip-packages/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f45e157",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T15:11:54.422054Z",
     "iopub.status.busy": "2023-08-09T15:11:54.421620Z",
     "iopub.status.idle": "2023-08-09T15:12:15.504054Z",
     "shell.execute_reply": "2023-08-09T15:12:15.502581Z"
    },
    "papermill": {
     "duration": 21.124158,
     "end_time": "2023-08-09T15:12:15.507282",
     "exception": false,
     "start_time": "2023-08-09T15:11:54.383124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "##### Fold_1\n",
      "Loss: 0.44952469194926603\n",
      "##### Fold_2\n",
      "Loss: 0.11128156149906934\n",
      "##### Fold_3\n",
      "Loss: 0.14789071859328523\n",
      "##### Fold_4\n",
      "Loss: 0.14473490064391187\n",
      "##### Fold_5\n",
      "Loss: 0.15094455133133186\n",
      "(Best Performance [Fold_2]) Loss: 0.11128156149906934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id  class_0  class_1\n",
       "0  00eed32682bb      0.5      0.5\n",
       "1  010ebe33f668      0.5      0.5\n",
       "2  02fa521e1838      0.5      0.5\n",
       "3  040e15f562a2      0.5      0.5\n",
       "4  046e85c7cc7f      0.5      0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Improved version of https://www.kaggle.com/code/maximecapelle/ensemble-xgboost-tabpfn-without-normalization?scriptVersionId=137415102\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Import Preprocessing tools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#Import Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import xgboost as xgb\n",
    "from tabpfn import TabPFNClassifier\n",
    "\n",
    "#Import training libraries\n",
    "import tqdm\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "device_name = 'cuda:0'\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "# Data & Submission\n",
    "DATA_PATH = '/kaggle/input/icr-identify-age-related-conditions'\n",
    "SUBMISSION_PATH = '/kaggle/working/tabpfn_fixed0.17.csv'\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "DROP_COLUMNS = ['Id', 'Class']\n",
    "\n",
    "#Models\n",
    "IMPUTE_STRAT = 'median'\n",
    "KFOLD_SPLITS = 5\n",
    "CLASSIFIERS = [\n",
    "    xgb.XGBClassifier(n_estimators=100,max_depth=3,learning_rate=0.2,subsample=0.9,colsample_bytree=0.85),\n",
    "    xgb.XGBClassifier(),\n",
    "    TabPFNClassifier(N_ensemble_configurations=24,device = device_name),\n",
    "    TabPFNClassifier(N_ensemble_configurations=64,device = device_name)]\n",
    "\n",
    "# CLASSIFIERS = [TabPFNClassifier(N_ensemble_configurations=64)]\n",
    "    \n",
    "#Load in train and test data\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n",
    "greeks_df  = pd.read_csv(os.path.join(DATA_PATH, 'greeks.csv'))\n",
    "\n",
    "FEATURE_COLUMNS = [i for i in df.columns if i not in DROP_COLUMNS]\n",
    "FEATURE_COLUMNS_GREEKS = [i for i in greeks_df.columns if i not in DROP_COLUMNS]\n",
    "BinaryCategory = df.EJ.unique()[0]\n",
    "\n",
    "# .eq(): Goes through the dataframe and sets a value if they are the same or not. (Can choose astype())\n",
    "df.EJ = df.EJ.eq(BinaryCategory).astype('int')\n",
    "test_df.EJ = test_df.EJ.eq(BinaryCategory).astype('int')\n",
    "from datetime import date, datetime\n",
    "times = greeks_df.Epsilon.copy()\n",
    "times[greeks_df.Epsilon != 'Unknown'] = greeks_df.Epsilon[greeks_df.Epsilon != 'Unknown'].map(lambda x: datetime.strptime(x,'%m/%d/%Y').toordinal())\n",
    "times[greeks_df.Epsilon == 'Unknown'] = np.nan\n",
    "\n",
    "df = pd.concat((df, times), 1)\n",
    "test_df = np.array(test_df[FEATURE_COLUMNS])\n",
    "test_df_extra = np.concatenate((test_df, np.zeros((len(test_df),1)) + df.Epsilon.max()+1),1)\n",
    "# KF = KFold(n_splits = KFOLD_SPLITS, shuffle=True)\n",
    "KF = StratifiedKFold(n_splits=KFOLD_SPLITS, random_state=118, shuffle=True)\n",
    "\n",
    "\n",
    "class Ensemble():\n",
    "    def __init__(self, IMPUTE_STRAT, CLASSIFIERS):\n",
    "        self.imputer = SimpleImputer(missing_values=np.nan, strategy=IMPUTE_STRAT)\n",
    "        self.classifiers = CLASSIFIERS\n",
    "\n",
    "    #Train all classifiers independantly   \n",
    "    def fit(self, X, y):\n",
    "        cls, y = np.unique(y, return_inverse=True)\n",
    "        self.classes_ = cls\n",
    "        X = self.imputer.fit_transform(X)\n",
    "        for cl in self.classifiers:\n",
    "            cl.fit(X,y)\n",
    "     \n",
    "    def predict_proba(self, X):\n",
    "        X = self.imputer.transform(X)\n",
    "        model_probabilities = np.stack([classifier.predict_proba(X) for classifier in self.classifiers])\n",
    "        averaged_probabilities = np.mean(model_probabilities, axis=0)\n",
    "        \n",
    "        class_0_est_instances = averaged_probabilities[:, 0].sum()\n",
    "        Rest_est_instances = averaged_probabilities[:, 1:].sum()\n",
    "        # Weighted probabilities based on class imbalance\n",
    "        new_probabilities = averaged_probabilities * np.array([[1/(class_0_est_instances if i==0 else Rest_est_instances) for i in range(averaged_probabilities.shape[1])]])\n",
    "        return new_probabilities / np.sum(new_probabilities, axis=1, keepdims=1)\n",
    "# Loop through each fold\n",
    "Losses = []\n",
    "MODELS = {}\n",
    "\n",
    "FEATURE_COLUMNS = [i for i in df.columns if i not in DROP_COLUMNS]\n",
    "\n",
    "def MakeDatasets(df, train_index, valid_index, FEATURE_COLUMNS):\n",
    "        # Fetch values corresponding to the index \n",
    "        train_df = df.iloc[train_index]\n",
    "        valid_df = df.iloc[valid_index]\n",
    "        \n",
    "        # Select only feature columns for training.\n",
    "        \n",
    "        X_train = train_df[FEATURE_COLUMNS]\n",
    "        X_val = valid_df[FEATURE_COLUMNS]\n",
    "        y_train = train_df['Class']        \n",
    "        y_val = valid_df['Class']\n",
    "\n",
    "        return X_train, X_val, y_train, y_val \n",
    "\n",
    "\n",
    "def Run_Training(df, Losses, model):\n",
    "#         for i, (train_index, valid_index) in enumerate(KF.split(X=df)):\n",
    "        for i, (train_index, valid_index) in enumerate(KF.split(df, df['Class'])):\n",
    "\n",
    "                print(f'##### Fold_{i+1}')\n",
    "\n",
    "                #Make Datasets\n",
    "                X_train, X_val, y_train, y_val = MakeDatasets(df, train_index, valid_index, FEATURE_COLUMNS)\n",
    "                \n",
    "                #Train Model        \n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                #Store the Model in Dictionary\n",
    "                MODELS[f\"fold_{i+1}\"] = model\n",
    "                \n",
    "                #Predict classes of validation data\n",
    "                y_pred = model.predict_proba(X_val)\n",
    "\n",
    "                loss = balanced_log_loss(y_val, y_pred)\n",
    "                print(f\"Loss: {loss}\")\n",
    "                Losses.append(loss)\n",
    "        \n",
    "        # Choose best model\n",
    "        BestModelIndex = np.argmin(Losses)\n",
    "        BestModel = MODELS[f\"fold_{BestModelIndex+1}\"]\n",
    "        print(f'(Best Performance [Fold_{BestModelIndex+1}]) Loss: {Losses[BestModelIndex]}')\n",
    "\n",
    "        return BestModel\n",
    "Ens = Ensemble(IMPUTE_STRAT, CLASSIFIERS)\n",
    "df['Epsilon'] = df.Epsilon.astype(float)\n",
    "BestModel = Run_Training(df, Losses, Ens)\n",
    "\n",
    "tabpfn_xgboost = BestModel.predict_proba(test_df_extra)\n",
    "print(tabpfn_xgboost)\n",
    "\n",
    "sample_submission = pd.read_csv(os.path.join(DATA_PATH, \"sample_submission.csv\"))\n",
    "sample_submission[['class_0', 'class_1']] = tabpfn_xgboost\n",
    "sample_submission.to_csv(SUBMISSION_PATH, index=False)\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6d211",
   "metadata": {
    "papermill": {
     "duration": 0.036912,
     "end_time": "2023-08-09T15:12:15.584676",
     "exception": false,
     "start_time": "2023-08-09T15:12:15.547764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# model 4 -> tabpfn+xgboost int "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96d80c7",
   "metadata": {
    "papermill": {
     "duration": 0.037143,
     "end_time": "2023-08-09T15:12:15.658839",
     "exception": false,
     "start_time": "2023-08-09T15:12:15.621696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "0.14 lb -> https://www.kaggle.com/code/jerryzheng111/icr-xgb-tabpfn-2kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8558ec57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T15:12:15.736536Z",
     "iopub.status.busy": "2023-08-09T15:12:15.736143Z",
     "iopub.status.idle": "2023-08-09T15:16:17.613159Z",
     "shell.execute_reply": "2023-08-09T15:16:17.611938Z"
    },
    "papermill": {
     "duration": 241.92039,
     "end_time": "2023-08-09T15:16:17.616873",
     "exception": false,
     "start_time": "2023-08-09T15:12:15.696483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.269787996170443\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Now for outer fold 1:\n",
      "Inner_fold = 1.0, val_loss = 0.02166\n",
      "Inner_fold = 2.0, val_loss = 0.02109\n",
      "Inner_fold = 3.0, val_loss = 0.02444\n",
      "Inner_fold = 4.0, val_loss = 0.01819\n",
      "Inner_fold = 5.0, val_loss = 0.02331\n",
      "80% Train Loss: 0.021675416863542032; Train Acc: 0.9981572481572482\n",
      "20% Val Loss: 0.1755891333759563; Val Acc: 0.9112903225806451\n",
      "\n",
      "Now for outer fold 2:\n",
      "Inner_fold = 1.0, val_loss = 0.01819\n",
      "Inner_fold = 2.0, val_loss = 0.01579\n",
      "Inner_fold = 3.0, val_loss = 0.03180\n",
      "Inner_fold = 4.0, val_loss = 0.01586\n",
      "Inner_fold = 5.0, val_loss = 0.02813\n",
      "80% Train Loss: 0.021835462233916275; Train Acc: 0.9993932038834952\n",
      "20% Val Loss: 0.18057189059271497; Val Acc: 0.9274193548387096\n",
      "\n",
      "Now for outer fold 3:\n",
      "Inner_fold = 1.0, val_loss = 0.01606\n",
      "Inner_fold = 2.0, val_loss = 0.02178\n",
      "Inner_fold = 3.0, val_loss = 0.02538\n",
      "Inner_fold = 4.0, val_loss = 0.02233\n",
      "Inner_fold = 5.0, val_loss = 0.02115\n",
      "80% Train Loss: 0.021599818216768756; Train Acc: 0.9981751824817519\n",
      "20% Val Loss: 0.3284661269652362; Val Acc: 0.926829268292683\n",
      "\n",
      "Now for outer fold 4:\n",
      "Inner_fold = 1.0, val_loss = 0.03050\n",
      "Inner_fold = 2.0, val_loss = 0.01477\n",
      "Inner_fold = 3.0, val_loss = 0.02386\n",
      "Inner_fold = 4.0, val_loss = 0.01469\n",
      "Inner_fold = 5.0, val_loss = 0.04334\n",
      "80% Train Loss: 0.025786216385598786; Train Acc: 0.9962871287128713\n",
      "20% Val Loss: 0.1981985024219179; Val Acc: 0.9105691056910569\n",
      "\n",
      "Now for outer fold 5:\n",
      "Inner_fold = 1.0, val_loss = 0.01494\n",
      "Inner_fold = 2.0, val_loss = 0.01233\n",
      "Inner_fold = 3.0, val_loss = 0.02281\n",
      "Inner_fold = 4.0, val_loss = 0.03590\n",
      "Inner_fold = 5.0, val_loss = 0.00987\n",
      "80% Train Loss: 0.020264131906595814; Train Acc: 0.9975124378109452\n",
      "20% Val Loss: 0.26526177670992973; Val Acc: 0.8861788617886179\n",
      "\n",
      "CPU times: user 4min 45s, sys: 1.59 s, total: 4min 47s\n",
      "Wall time: 4min 1s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id  class_0  class_1\n",
       "0  00eed32682bb      0.5      0.5\n",
       "1  010ebe33f668      0.5      0.5\n",
       "2  02fa521e1838      0.5      0.5\n",
       "3  040e15f562a2      0.5      0.5\n",
       "4  046e85c7cc7f      0.5      0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder,normalize\n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import xgboost\n",
    "import inspect\n",
    "from collections import defaultdict\n",
    "from tabpfn import TabPFNClassifier\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "run_para = 'kaggle'\n",
    "if run_para == 'kaggle':\n",
    "    train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n",
    "    test = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\n",
    "    sample = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "    greeks = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/greeks.csv')\n",
    "elif run_para == 'local':\n",
    "    train = pd.read_csv('./icr-identify-age-related-conditions/train.csv')\n",
    "    test = pd.read_csv('./icr-identify-age-related-conditions/test.csv')\n",
    "    sample = pd.read_csv('./icr-identify-age-related-conditions/sample_submission.csv')\n",
    "    greeks = pd.read_csv('./icr-identify-age-related-conditions/greeks.csv')\n",
    "    \n",
    "first_category = train.EJ.unique()[0]\n",
    "train.EJ = train.EJ.eq(first_category).astype('int')\n",
    "test.EJ = test.EJ.eq(first_category).astype('int')\n",
    "int_denominators = {\n",
    "    'AB': 0.004273,\n",
    "    'AF': 0.00242,\n",
    "    'AH': 0.008709,\n",
    "    'AM': 0.003097,\n",
    "    'AR': 0.005244,\n",
    "    'AX': 0.008859,\n",
    "    'AY': 0.000609,\n",
    "    'AZ': 0.006302,\n",
    "    'BC': 0.007028,\n",
    "    'BD ': 0.00799,\n",
    "    'BN': 0.3531,\n",
    "    'BP': 0.004239,\n",
    "    'BQ': 0.002605,\n",
    "    'BR': 0.006049,\n",
    "    'BZ': 0.004267,\n",
    "    'CB': 0.009191,\n",
    "    'CC': 6.12e-06,\n",
    "    'CD ': 0.007928,\n",
    "    'CF': 0.003041,\n",
    "    'CH': 0.000398,\n",
    "    'CL': 0.006365,\n",
    "    'CR': 7.5e-05,\n",
    "    'CS': 0.003487,\n",
    "    'CU': 0.005517,\n",
    "    'CW ': 9.2e-05,\n",
    "    'DA': 0.00388,\n",
    "    'DE': 0.004435,\n",
    "    'DF': 0.000351,\n",
    "    'DH': 0.002733,\n",
    "    'DI': 0.003765,\n",
    "    'DL': 0.00212,\n",
    "    'DN': 0.003412,\n",
    "    'DU': 0.0013794,\n",
    "    'DV': 0.00259,\n",
    "    'DY': 0.004492,\n",
    "    'EB': 0.007068,\n",
    "    'EE': 0.004031,\n",
    "    'EG': 0.006025,\n",
    "    'EH': 0.006084,\n",
    "    'EL': 0.000429,\n",
    "    'EP': 0.009269,\n",
    "    'EU': 0.005064,\n",
    "    'FC': 0.005712,\n",
    "    'FD ': 0.005937,\n",
    "    'FE': 0.007486,\n",
    "    'FI': 0.005513,\n",
    "    'FR': 0.00058,\n",
    "    'FS': 0.006773,\n",
    "    'GB': 0.009302,\n",
    "    'GE': 0.004417,\n",
    "    'GF': 0.004374,\n",
    "    'GH': 0.003721,\n",
    "    'GI': 0.002572\n",
    "}\n",
    "for k, v in int_denominators.items():\n",
    "    train[k] = np.round(train[k] / v, 1)\n",
    "    test[k] = np.round(test[k] / v, 1)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 5\n",
    "if run_para == 'kaggle':\n",
    "    BNpd = pd.concat([train['BN'], test['BN']], axis=0, ignore_index=True)\n",
    "elif run_para == 'local':\n",
    "    BNpd = train['BN']\n",
    "\n",
    "BNpd = pd.concat([train['BN'], test['BN']], axis=0, ignore_index=True)\n",
    "data = BNpd.values.reshape(-1, 1)\n",
    "kmodel = KMeans(n_clusters=k)           # k\n",
    "kmodel.fit(data)  # \n",
    "c = pd.DataFrame(kmodel.cluster_centers_, columns=['cc']) #\n",
    "c0 = pd.DataFrame({'cc': [0.0]})\n",
    "c = pd.concat([c0, c], axis=0, ignore_index=True)\n",
    "c = c.sort_values(by='cc').reset_index(drop=True)\n",
    "\n",
    "for i in range(c.shape[0] - 1):\n",
    "    c.iloc[i]['cc'] = (c.iloc[i]['cc'] + c.iloc[i+1]['cc']) / 2\n",
    "c = c.drop(c.index[-1])\n",
    "\n",
    "c0 = pd.DataFrame({'cc': [0.0]})\n",
    "cn = pd.DataFrame({'cc': [max(train['BN'].max(), test['BN'].max()) * 5]})\n",
    "c = pd.concat([c0, c, cn], axis=0, ignore_index=True)\n",
    "c = c['cc'].round().astype(int)\n",
    "c = c.unique()\n",
    "range_num = c.shape[0] - 1\n",
    "c = c.tolist()\n",
    "\n",
    "train_BN = train['BN'].values\n",
    "train_binning = pd.cut(train_BN, c, labels=range(range_num), include_lowest=True)\n",
    "train['BN_binning'] = train_binning\n",
    "\n",
    "test_BN = test['BN'].values\n",
    "test_binning = pd.cut(test_BN, c, labels=range(range_num), include_lowest=True)\n",
    "test['BN_binning'] = test_binning\n",
    "predictor_columns = [n for n in train.columns if n != 'Class' and n != 'Id']\n",
    "from datetime import datetime\n",
    "times = greeks.Epsilon.copy()\n",
    "times[greeks.Epsilon != 'Unknown'] = greeks.Epsilon[greeks.Epsilon != 'Unknown'].map(lambda x: datetime.strptime(x,'%m/%d/%Y').toordinal())\n",
    "times[greeks.Epsilon == 'Unknown'] = np.nan\n",
    "train_pred_and_time = pd.concat((train, times, greeks.Alpha), axis=1)\n",
    "train_cate = train_pred_and_time.iloc[:, -1]        # A, B, D, G\n",
    "train_pred_and_time = train_pred_and_time.drop(train_pred_and_time.columns[-1], axis=1)\n",
    "\n",
    "test_predictors = test[predictor_columns]\n",
    "test_time = np.zeros((len(test_predictors), 1)) + train_pred_and_time.Epsilon.max() + 1\n",
    "test_pred_and_time = pd.concat((test_predictors, pd.DataFrame(test_time, columns=['Epsilon'])), axis=1)\n",
    "y_true = np.array([1,1,1,0,0,0]).astype('int')\n",
    "y_pred = np.array([1] * len(y_true)).astype('float64')\n",
    "bll = balanced_log_loss(y_true, y_pred)\n",
    "print(bll)\n",
    "class Ensemble():\n",
    "    def __init__(self):\n",
    "        self.imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "        self.classifiers =[xgboost.XGBClassifier(n_estimators=100,max_depth=3,learning_rate=0.2,subsample=0.9,colsample_bytree=0.85),\n",
    "                           xgboost.XGBClassifier(),\n",
    "\n",
    "                           TabPFNClassifier(N_ensemble_configurations=24,device = device_name),\n",
    "                           TabPFNClassifier(N_ensemble_configurations=64,device = device_name)]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y = y.values\n",
    "        unique_classes, y = np.unique(y, return_inverse=True)\n",
    "        self.classes_ = unique_classes\n",
    "        # first_category = X.EJ.unique()[0]\n",
    "        # X.EJ = X.EJ.eq(first_category).astype('int')\n",
    "        \n",
    "        X = self.imputer.fit_transform(X)\n",
    "        for classifier in self.classifiers:\n",
    "            if classifier == self.classifiers[2] or classifier == self.classifiers[3]:\n",
    "                classifier.fit(X,y,overwrite_warning =True)\n",
    "            else :\n",
    "                classifier.fit(X, y)\n",
    "     \n",
    "    def predict_proba(self, x):\n",
    "        x = self.imputer.transform(x)\n",
    "\n",
    "        probabilities = np.stack([classifier.predict_proba(x) for classifier in self.classifiers])\n",
    "        averaged_probabilities = np.mean(probabilities, axis=0)\n",
    "        class_0_est_instances = averaged_probabilities[:, 0].sum()\n",
    "        others_est_instances = averaged_probabilities[:, 1:].sum()\n",
    "        \n",
    "        # Weighted probabilities based on class imbalance\n",
    "        new_probabilities = averaged_probabilities * np.array([[1/(class_0_est_instances if i==0 else others_est_instances) for i in range(averaged_probabilities.shape[1])]])\n",
    "        \n",
    "        return new_probabilities / np.sum(new_probabilities, axis=1, keepdims=1) \n",
    "from sklearn.model_selection import KFold as KF, GridSearchCV\n",
    "\n",
    "cv_outer = KF(n_splits = 5, shuffle=True, random_state=19)\n",
    "cv_inner = KF(n_splits = 5, shuffle=True, random_state=19)\n",
    "# \n",
    "\n",
    "def calc_acc(y_pred, y):\n",
    "    probabilities = np.concatenate((y_pred[:, :1], np.sum(y_pred[:, 1:], 1, keepdims=True)), axis=1)\n",
    "    p0 = probabilities[:, :1]       # class=0\n",
    "    p1 = 1 - p0\n",
    "    \n",
    "    y = y.values.astype(int)\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(len(p0)):\n",
    "        if p0[i] >= p1[i]:\n",
    "            lab = 0\n",
    "        else :\n",
    "            lab = 1\n",
    "\n",
    "        if lab == y[i]:\n",
    "            cnt += 1\n",
    "\n",
    "    return cnt / len(p0)\n",
    "# balanced log loss\n",
    "\n",
    "def calc_loss(y_pred, y):\n",
    "    probabilities = np.concatenate((y_pred[:,:1], np.sum(y_pred[:, 1:], 1, keepdims=True)), axis=1)\n",
    "    p0 = probabilities[:, :1]       # class=0\n",
    "#     p0[p0 > 0.80] = 1\n",
    "#     p0[p0 < 0.20] = 0\n",
    "\n",
    "    p1 = 1 - p0\n",
    "    \n",
    "    y = y.values.astype(int)\n",
    "    loss = balanced_log_loss(y, p1)\n",
    "\n",
    "    return loss\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "def training(model, x, y, y_meta):\n",
    "    low_loss = np.inf\n",
    "    best_models = []\n",
    "    for out_id, (train_idx, val_idx) in enumerate(cv_outer.split(x), start=1):\n",
    "        print(f'Now for outer fold {out_id}:')\n",
    "        x_train_ori, x_val = x.iloc[train_idx], x.iloc[val_idx]\n",
    "        y_train_ori, y_val = y_meta.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        x_train, y_train = ros.fit_resample(x_train_ori, y_train_ori)\n",
    "        # x_train, y_train = x_train_ori, y_train_ori\n",
    "        \n",
    "        train_loss = np.zeros((x_train.shape[0], 4))\n",
    "    \n",
    "        out_X, out_y_meta = x_train, y_train\n",
    "        out_y = out_y_meta.apply(lambda x: 0 if x == 'A' else 1)\n",
    "        \n",
    "        models = []\n",
    "\n",
    "        for in_id, (train_idx1, val_idx1) in enumerate(cv_inner.split(out_X), start=1):\n",
    "            in_x_train, in_x_val = out_X.iloc[train_idx1], out_X.iloc[val_idx1]\n",
    "            in_y_train, in_y_val = out_y_meta.iloc[train_idx1], out_y.iloc[val_idx1]\n",
    "\n",
    "            model.fit(in_x_train, in_y_train)\n",
    "            models.append(model) \n",
    "\n",
    "            y_pred = model.predict_proba(in_x_val)\n",
    "            train_loss[val_idx1] = y_pred\n",
    "\n",
    "            metric = calc_loss(y_pred, in_y_val)\n",
    "            print('Inner_fold = %.1f, val_loss = %.5f' % (in_id, metric))\n",
    "        \n",
    "        # modelsx_valloss\n",
    "        val_y_pred = np.zeros((x_val.shape[0], 4))\n",
    "        for model in models:\n",
    "            y_pred = model.predict_proba(x_val)\n",
    "            val_y_pred += y_pred\n",
    "        val_y_pred /= len(models)\n",
    "\n",
    "        metric_train = calc_loss(train_loss, out_y)\n",
    "        acc_train = calc_acc(train_loss, out_y)\n",
    "        print(f'80% Train Loss: {metric_train}; Train Acc: {acc_train}')\n",
    "        metric_val = calc_loss(val_y_pred, y_val)\n",
    "        acc_val = calc_acc(val_y_pred, y_val)\n",
    "        print(f'20% Val Loss: {metric_val}; Val Acc: {acc_val}\\n')\n",
    "\n",
    "        if metric_val < low_loss:\n",
    "            low_loss = metric_val\n",
    "            best_models = models\n",
    "\n",
    "        # break       #     \n",
    "        \n",
    "    return best_models\n",
    "x_ = train_pred_and_time.drop(['Class', 'Id'], axis=1)\n",
    "y_ = train_pred_and_time.Class\n",
    "y_meta_ = train_cate\n",
    "yt = Ensemble()\n",
    "\n",
    "models = training(yt, x_, y_, y_meta_)\n",
    "# import pickle\n",
    "\n",
    "# for cnt, model in enumerate(models, start=1):\n",
    "#     filename = 'model' + str(cnt) + '.pkl'\n",
    "#     with open(filename, 'wb') as f:\n",
    "#         pickle.dump(model, f)\n",
    "\n",
    "y_pred = np.zeros((test_pred_and_time.shape[0], 4))\n",
    "for model in models:\n",
    "    y_pred += model.predict_proba(test_pred_and_time)\n",
    "y_pred = y_pred / len(models)\n",
    "\n",
    "probabilities = np.concatenate((y_pred[:,:1], np.sum(y_pred[:,1:], 1, keepdims=True)), axis=1)\n",
    "p0 = probabilities[:,:1]\n",
    "submission = pd.DataFrame(test[\"Id\"], columns=[\"Id\"])\n",
    "submission[\"class_0\"] = p0\n",
    "submission[\"class_1\"] = 1 - p0\n",
    "submission.to_csv('tabpfn14.csv', index=False)\n",
    "submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed26086",
   "metadata": {
    "papermill": {
     "duration": 0.043733,
     "end_time": "2023-08-09T15:16:17.708365",
     "exception": false,
     "start_time": "2023-08-09T15:16:17.664632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# model 5 -> fixed lightgbm public baseline lb 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d41102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T15:16:17.792435Z",
     "iopub.status.busy": "2023-08-09T15:16:17.792031Z",
     "iopub.status.idle": "2023-08-09T15:16:18.136528Z",
     "shell.execute_reply": "2023-08-09T15:16:18.135342Z"
    },
    "papermill": {
     "duration": 0.390549,
     "end_time": "2023-08-09T15:16:18.140299",
     "exception": false,
     "start_time": "2023-08-09T15:16:17.749750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedGroupKFold\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class FeatureCreator():\n",
    "    def __init__(self, add_attributes=True):\n",
    "        \n",
    "        self.add_attributes = add_attributes\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        if self.add_attributes:\n",
    "            X_copy = X.copy()\n",
    "            \n",
    "            #X_copy = self.fill_na(X)\n",
    "            X_copy = self.encoder(X_copy)\n",
    "            X_copy = self.base(X_copy)\n",
    "            return X_copy\n",
    "        else:\n",
    "            return X_copy\n",
    "    def fill_na(self, X):\n",
    "        return X.fillna(X.median())\n",
    "    \n",
    "    def base(self , X):\n",
    "        try:\n",
    "            X['out_GL'] = 0\n",
    "            X.loc[X['GL']<1,'out_GL'] = X.loc[X['GL']<1,'GL'].map(lambda x : x-X.loc[X['GL']<1,'GL'].mean())\n",
    "            X.loc[X['GL']>1.5,'out_GL'] = X.loc[X['GL']>1.5,'GL'].map(lambda x : x-X.loc[X['GL']>1.5,'GL'].mean())\n",
    "            X.out_GL = X.out_GL.astype('float')\n",
    "            X['DA*CS'] = np.log(X.DA*2 / X.CS**0.5)\n",
    "\n",
    "            return X\n",
    "        except:\n",
    "            print('have missing columns')\n",
    "            return X\n",
    "    def encoder(self, X):\n",
    "        try:\n",
    "            X['EJ'] = X['EJ'].map({'A': 0, 'B': 1})\n",
    "            return X\n",
    "        except:\n",
    "            return X\n",
    "import os\n",
    "import random\n",
    "def seed_everything(seed=None):\n",
    "    '''\n",
    "\n",
    "    seed\n",
    "    :param seed: int, \n",
    "    '''\n",
    "    max_seed_value = np.iinfo(np.uint32).max\n",
    "    min_seed_value = np.iinfo(np.uint32).min\n",
    "\n",
    "    if (seed is None) or not (min_seed_value <= seed <= max_seed_value):\n",
    "        seed = random.randint(np.iinfo(np.uint32).min, np.iinfo(np.uint32).max)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    return seed\n",
    "seed_everything(42)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/test.csv\")\n",
    "greeks = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/greeks.csv\")\n",
    "\n",
    "ft = ['Id','AB','BQ','FL','DU','DA','CR','FI','EE','DY','FD ','GL','DL','EL','CC','AF','BC','FR','DI','FC','EP','DN','CH','CD ','AM','DH','EB','EH','EU','CU','DE','GE','CL','FS','CS','GF','FE','EG','CB']\n",
    "\n",
    "FT = FeatureCreator()\n",
    "\n",
    "train = FT.transform(train)\n",
    "test = FT.transform(test)\n",
    "\n",
    "drop_ft = ['DI','FD ','CS'] \n",
    "train = train.drop(columns = drop_ft)#,'CU','DY','CB'\n",
    "test = test.drop(columns = drop_ft)\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n",
    "    nc = np.bincount(y_true.astype(int))\n",
    "    logloss = (-1/nc[0]*(np.sum(np.where(y_true==0,1,0) * np.log(1-y_pred))) - 1/nc[1]*(np.sum(np.where(y_true!=0,1,0) * np.log(y_pred)))) / 2\n",
    "    return logloss\n",
    "def lgb_metric(y_true, y_pred):\n",
    "    return 'balanced_log_loss', balanced_log_loss(y_true, y_pred), False\n",
    "\n",
    "class BalancedLoglossObjective(object):\n",
    "    def calc_ders_range(self, approxes, targets):\n",
    "        prob = 1.0 / (1 + np.exp(-np.array(approxes)))\n",
    "        nc = np.bincount(np.array(targets).astype(int))\n",
    "        der1 = np.where(np.array(targets) == 0, -1 / nc[0] * (1 - prob), 1 / nc[1] * prob)\n",
    "        der2 = np.where(np.array(targets) == 0, -1 / nc[0] * prob, 1 / nc[1] * (prob - 1) * prob)\n",
    "        return list(zip(der1, der2))\n",
    "\n",
    "class BalancedLoglossMetric(object):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return error / (weight + 1e-38)\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        return False\n",
    "\n",
    "    def evaluate(self, approxes, targets, weight):\n",
    "        approxes = np.clip(approxes, 1e-15, 1-1e-15)\n",
    "        nc = np.bincount(approxes.astype(int))\n",
    "        logloss = (-1/nc[0]*(np.sum(np.where(targets==0,1,0) * np.log(1-approxes))) - 1/nc[1]*(np.sum(np.where(y_true!=0,1,0) * np.log(approxes)))) / 2\n",
    "        return logloss\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score,f1_score,precision_score,recall_score,accuracy_score\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "import lightgbm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import plot_metric\n",
    "import random\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "from functools import partial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "761a820a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T15:16:18.228166Z",
     "iopub.status.busy": "2023-08-09T15:16:18.227747Z",
     "iopub.status.idle": "2023-08-09T15:16:22.862350Z",
     "shell.execute_reply": "2023-08-09T15:16:22.861133Z"
    },
    "papermill": {
     "duration": 4.68123,
     "end_time": "2023-08-09T15:16:22.866411",
     "exception": false,
     "start_time": "2023-08-09T15:16:18.185181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------> Fold 1 <-----------------\n",
      "|------------------------------------|\n",
      "|                                    |\n",
      "|            LightGBM                |\n",
      "|                                    |\n",
      "|------------------------------------|\n",
      " \n",
      "------------- Train -----------------\n",
      "BLL: 0.23655023126337382\n",
      "------------> Fold 2 <-----------------\n",
      "|------------------------------------|\n",
      "|                                    |\n",
      "|            LightGBM                |\n",
      "|                                    |\n",
      "|------------------------------------|\n",
      " \n",
      "------------- Train -----------------\n",
      "BLL: 0.08871381298461571\n",
      "------------> Fold 3 <-----------------\n",
      "|------------------------------------|\n",
      "|                                    |\n",
      "|            LightGBM                |\n",
      "|                                    |\n",
      "|------------------------------------|\n",
      " \n",
      "------------- Train -----------------\n",
      "BLL: 0.28279257535637264\n",
      "------------> Fold 4 <-----------------\n",
      "|------------------------------------|\n",
      "|                                    |\n",
      "|            LightGBM                |\n",
      "|                                    |\n",
      "|------------------------------------|\n",
      " \n",
      "------------- Train -----------------\n",
      "BLL: 0.12457403006062567\n",
      "------------> Fold 5 <-----------------\n",
      "|------------------------------------|\n",
      "|                                    |\n",
      "|            LightGBM                |\n",
      "|                                    |\n",
      "|------------------------------------|\n",
      " \n",
      "------------- Train -----------------\n",
      "BLL: 0.1185094681969215\n",
      "------------> Fold 6 <-----------------\n",
      "|------------------------------------|\n",
      "|                                    |\n",
      "|            LightGBM                |\n",
      "|                                    |\n",
      "|------------------------------------|\n",
      " \n",
      "------------- Train -----------------\n",
      "BLL: 0.1499491587428825\n",
      "------------> Fold 7 <-----------------\n",
      "|------------------------------------|\n",
      "|                                    |\n",
      "|            LightGBM                |\n",
      "|                                    |\n",
      "|------------------------------------|\n",
      " \n",
      "------------- Train -----------------\n",
      "BLL: 0.39579960604443976\n",
      "------------> Fold 8 <-----------------\n",
      "|------------------------------------|\n",
      "|                                    |\n",
      "|            LightGBM                |\n",
      "|                                    |\n",
      "|------------------------------------|\n",
      " \n",
      "------------- Train -----------------\n",
      "BLL: 0.162031621240772\n",
      "------------> Fold 9 <-----------------\n",
      "|------------------------------------|\n",
      "|                                    |\n",
      "|            LightGBM                |\n",
      "|                                    |\n",
      "|------------------------------------|\n",
      " \n",
      "------------- Train -----------------\n",
      "BLL: 0.16772303164807403\n",
      "------------> Fold 10 <-----------------\n",
      "|------------------------------------|\n",
      "|                                    |\n",
      "|            LightGBM                |\n",
      "|                                    |\n",
      "|------------------------------------|\n",
      " \n",
      "------------- Train -----------------\n",
      "BLL: 0.1549528486122275\n",
      "feature is ['AB' 'AF' 'AH' 'AM' 'AR' 'AX' 'AY' 'AZ' 'BC' 'BD ' 'BN' 'BP' 'BQ' 'BR'\n",
      " 'BZ' 'CB' 'CC' 'CD ' 'CF' 'CH' 'CL' 'CR' 'CU' 'CW ' 'DA' 'DE' 'DF' 'DH'\n",
      " 'DL' 'DN' 'DU' 'DV' 'DY' 'EB' 'EE' 'EG' 'EH' 'EJ' 'EL' 'EP' 'EU' 'FC'\n",
      " 'FE' 'FI' 'FL' 'FR' 'FS' 'GB' 'GE' 'GF' 'GH' 'GI' 'GL' 'out_GL' 'DA*CS']\n",
      "Average ctb is : 0.0 ; lgb is : 0.18815963841503053 ; xgb is : 0.0\n",
      "std is 0.08766881213272876\n",
      "total is 0.2758284505477593\n",
      "CPU times: user 4.46 s, sys: 165 ms, total: 4.63 s\n",
      "Wall time: 4.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def cv(train, test, target,kfold = None):\n",
    "    n_reapts = 1\n",
    "    random_state = 42\n",
    "    n_estimators = 99999\n",
    "    n_trials = 3500\n",
    "    early_stopping_rounds = 2000\n",
    "    verbose = False\n",
    "    device = 'cpu'\n",
    "    ensemble_score = []\n",
    "    fold_scores = []\n",
    "    weights = []\n",
    "    oof_each_predss = []\n",
    "    test_each_predss = []\n",
    "    ensemble_test = np.zeros(len(test))\n",
    "    oof_predss = np.zeros(len(train))\n",
    "    # Fix seed\n",
    "    random.seed(random_state)\n",
    "    random_state_list = random.sample(range(9999), n_reapts)\n",
    "    \n",
    "    test_preds = np.zeros((len(test)))\n",
    "    cv = 0\n",
    "    kf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=42)\n",
    "    train_result = train.Class.copy()\n",
    "    train_targets = train[target]\n",
    "    train = train.drop(columns = [target])\n",
    "    lgb_loss = 0\n",
    "    ctb_loss = 0\n",
    "    xgb_loss = 0\n",
    "    final_result = []\n",
    "    Ensemble_MAE = 0\n",
    "    score = []\n",
    "\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(train, train_targets)):\n",
    "        print(f\"------------> Fold {fold + 1} <-----------------\")\n",
    "\n",
    "        X_train, X_valid = train.iloc[train_idx], train.iloc[valid_idx]\n",
    "        y_train, y_valid = train_targets.iloc[train_idx], train_targets.iloc[valid_idx]\n",
    "        \n",
    "        class_weights = [1, len(y_train[y_train == 0]) / len(y_train[y_train == 1])]\n",
    "        \n",
    "\n",
    "        lgb = LGBMClassifier(boosting_type='goss', learning_rate=0.06733232950390658, n_estimators = 50000, \n",
    "                         early_stopping_round = 300, random_state=42,\n",
    "                        subsample=0.6970532011679706,\n",
    "                        colsample_bytree=0.6055755840633003,\n",
    "                         class_weight='balanced',\n",
    "                         metric='none', is_unbalance=True, max_depth=8)#,reg_alpha = 0.1\n",
    "        lgb.fit(X_train, y_train, eval_set=(X_valid, y_valid), verbose=0,eval_metric=lgb_metric)\n",
    "        # plot_metric(lgb,'mae')\n",
    "        lgb_preds = lgb.predict_proba(X_valid)\n",
    "        lgb_test_preds = lgb.predict_proba(test)\n",
    "        \n",
    "        print(f\"|------------------------------------|\")\n",
    "        print(f\"|                                    |\")\n",
    "        print(f\"|            LightGBM                |\")\n",
    "        print(f\"|                                    |\")\n",
    "        print(f\"|------------------------------------|\")\n",
    "        print(\" \")\n",
    "        print(f\"------------- Train -----------------\")\n",
    "        print(f\"BLL: {balanced_log_loss(y_valid,lgb_preds[:,1])}\")\n",
    "        lgb_loss += balanced_log_loss(y_valid[:],lgb_preds[:,1])\n",
    "        \n",
    "        ctb_preds = np.zeros(len(X_valid))\n",
    "\n",
    "        ctb_test_preds = np.zeros(len(test))\n",
    "#         xgb_test_preds = np.zeros(len(test))\n",
    "    \n",
    "        meta_train = [lgb_preds]#ctb_preds, , xgb_preds\n",
    "        meta_test = [lgb_test_preds]#ctb_test_preds, , xgb_test_preds\n",
    "        \n",
    "        final_result.append(meta_test)\n",
    "        train_result[valid_idx] = lgb_preds[:,1]\n",
    "        # weights.append(2/(lgb_loss+xgb_loss))\n",
    "        weights.append(1/(balanced_log_loss(y_valid,lgb_preds[:,1])))\n",
    "        score.append(balanced_log_loss(y_valid,lgb_preds[:,1]))\n",
    "    print(f'feature is {X_valid.columns.values}')\n",
    "    print(f\"Average ctb is : {ctb_loss/kfold} ; lgb is : {lgb_loss/kfold} ; xgb is : {xgb_loss/kfold}\")\n",
    "    print(f\"std is {np.std(score)}\")\n",
    "    print(f\"total is {np.std(score) + lgb_loss/kfold}\")\n",
    "    return train_result,final_result,ensemble_test,weights\n",
    "meta_train,meta_test,test_pred,weight = cv(train = train.drop(columns = ['Id']), test = test.drop(columns = ['Id']), target = 'Class',kfold=10)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77455f26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T15:16:22.953074Z",
     "iopub.status.busy": "2023-08-09T15:16:22.951914Z",
     "iopub.status.idle": "2023-08-09T15:16:22.974541Z",
     "shell.execute_reply": "2023-08-09T15:16:22.972756Z"
    },
    "papermill": {
     "duration": 0.069433,
     "end_time": "2023-08-09T15:16:22.977192",
     "exception": false,
     "start_time": "2023-08-09T15:16:22.907759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred>0.9 is 73 0.676\n",
      "pred<0.1 is 416 0.817\n"
     ]
    }
   ],
   "source": [
    "fit_value = pd.DataFrame({'pred':meta_train,'Class':train.Class})\n",
    "len(fit_value.loc[(fit_value['pred']>0.9) & (fit_value['Class']==1),:])\n",
    "print(f\"pred>0.9 is {len(fit_value.loc[(fit_value['pred']>0.9) & (fit_value['Class']==1),:])} {round(len(fit_value.loc[(fit_value['pred']>0.9) & (fit_value['Class']==1),:]) / len(fit_value.loc[fit_value.Class==1]),3)}\")\n",
    "print(f\"pred<0.1 is {len(fit_value.loc[(fit_value['pred']<0.1) & (fit_value['Class']==0),:])} {round(len(fit_value.loc[(fit_value['pred']<0.1) & (fit_value['Class']==0),:]) / len(fit_value.loc[fit_value.Class==0]),3)}\")\n",
    "test_preds = np.zeros((test.shape[0],2))\n",
    "for i in range(10):\n",
    "    test_preds[:, 0] += weight[i] * meta_test[i][0][:, 0]\n",
    "    test_preds[:, 1] += weight[i] * meta_test[i][0][:, 1]\n",
    "test_preds /= sum(weight)\n",
    "submission = pd.concat([test.Id,pd.DataFrame(test_preds,columns = ['class_0', 'class_1'])],axis=1)\n",
    "submission.to_csv(r\"public_lgb15.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e54020f",
   "metadata": {
    "papermill": {
     "duration": 0.043115,
     "end_time": "2023-08-09T15:16:23.068576",
     "exception": false,
     "start_time": "2023-08-09T15:16:23.025461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "based on the findings on this notebook https://www.kaggle.com/code/raddar/icr-competition-analysis-and-findings/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a116656",
   "metadata": {
    "papermill": {
     "duration": 0.041845,
     "end_time": "2023-08-09T15:16:23.152520",
     "exception": false,
     "start_time": "2023-08-09T15:16:23.110675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let me summarize the key findings from each chapter:\n",
    "\n",
    "Chapter 1: BN (age) is a crucial feature, and patients under the age of 44 (5% of the training data) are always Class 0 (healthy).\n",
    "\n",
    "Chapter 2: The column BQ is significant. If BQ is None, Class is always 0. This represents 10% of the population and indicates possible data drift.\n",
    "\n",
    "Chapter 3: The population is segmented by EJ. When EJ == A, some columns are constant, and EH is always 0.5. This means there is no reason to encode EJ, and it can be dropped from models. The mean Class=1 rate fluctuates less for EJ=A compared to EJ=B, suggesting data stratification.\n",
    "\n",
    "Chapter 4: Class=1 rate drifts over time. Typical cross-validation may not be valid due to the lack of real weights and varying class_weights for binary logloss components. Optimizing class_weights based on LB feedback may be worth considering.\n",
    "\n",
    "Chapter 5: Very few hard samples exist in the data, representing about 7% of positive samples. These hard samples have a significant impact on fold scores, making the problem challenging.\n",
    "\n",
    "Chapter 6: Public LB contains only a few hard cases, which suggests that testing models without overrides in the public LB is practically useless. Public LB's small number of hard cases may not be representative of the private test set.\n",
    "\n",
    "Chapter 7: Using pseudolabeling on the public LB and adopting a 2-stage modeling approach may be helpful for competition strategy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1d7a20a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T15:16:23.240205Z",
     "iopub.status.busy": "2023-08-09T15:16:23.239413Z",
     "iopub.status.idle": "2023-08-09T15:16:23.283074Z",
     "shell.execute_reply": "2023-08-09T15:16:23.281555Z"
    },
    "papermill": {
     "duration": 0.091044,
     "end_time": "2023-08-09T15:16:23.286154",
     "exception": false,
     "start_time": "2023-08-09T15:16:23.195110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the test.csv file\n",
    "COMP_PATH = \"/kaggle/input/icr-identify-age-related-conditions\"\n",
    "test = pd.read_csv(f\"{COMP_PATH}/test.csv\")\n",
    "\n",
    "int_denominators = {\n",
    " 'AB': 0.004273,\n",
    " 'AF': 0.00242,\n",
    " 'AH': 0.008709,\n",
    " 'AM': 0.003097,\n",
    " 'AR': 0.005244,\n",
    " 'AX': 0.008859,\n",
    " 'AY': 0.000609,\n",
    " 'AZ': 0.006302,\n",
    " 'BC': 0.007028,\n",
    " 'BD ': 0.00799,\n",
    " 'BN': 0.3531,\n",
    " 'BP': 0.004239,\n",
    " 'BQ': 0.002605,\n",
    " 'BR': 0.006049,\n",
    " 'BZ': 0.004267,\n",
    " 'CB': 0.009191,\n",
    " 'CC': 6.12e-06,\n",
    " 'CD ': 0.007928,\n",
    " 'CF': 0.003041,\n",
    " 'CH': 0.000398,\n",
    " 'CL': 0.006365,\n",
    " 'CR': 7.5e-05,\n",
    " 'CS': 0.003487,\n",
    " 'CU': 0.005517,\n",
    " 'CW ': 9.2e-05,\n",
    " 'DA': 0.00388,\n",
    " 'DE': 0.004435,\n",
    " 'DF': 0.000351,\n",
    " 'DH': 0.002733,\n",
    " 'DI': 0.003765,\n",
    " 'DL': 0.00212,\n",
    " 'DN': 0.003412,\n",
    " 'DU': 0.0013794,\n",
    " 'DV': 0.00259,\n",
    " 'DY': 0.004492,\n",
    " 'EB': 0.007068,\n",
    " 'EE': 0.004031,\n",
    " 'EG': 0.006025,\n",
    " 'EH': 0.006084,\n",
    " 'EL': 0.000429,\n",
    " 'EP': 0.009269,\n",
    " 'EU': 0.005064,\n",
    " 'FC': 0.005712,\n",
    " 'FD ': 0.005937,\n",
    " 'FE': 0.007486,\n",
    " 'FI': 0.005513,\n",
    " 'FR': 0.00058,\n",
    " 'FS': 0.006773,\n",
    " 'GB': 0.009302,\n",
    " 'GE': 0.004417,\n",
    " 'GF': 0.004374,\n",
    " 'GH': 0.003721,\n",
    " 'GI': 0.002572\n",
    "}\n",
    "for k, v in int_denominators.items():\n",
    "    test[k] = np.round(test[k]/v,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47240abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T15:16:23.376435Z",
     "iopub.status.busy": "2023-08-09T15:16:23.376019Z",
     "iopub.status.idle": "2023-08-09T15:16:23.410297Z",
     "shell.execute_reply": "2023-08-09T15:16:23.409125Z"
    },
    "papermill": {
     "duration": 0.080585,
     "end_time": "2023-08-09T15:16:23.413301",
     "exception": false,
     "start_time": "2023-08-09T15:16:23.332716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.120359\n",
       "1    0.120359\n",
       "2    0.120359\n",
       "3    0.120359\n",
       "4    0.120359\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "lgbm16 = pd.read_csv('/kaggle/working/lgbm0.16.csv')\n",
    "stackedxgbs15 = pd.read_csv('/kaggle/working/stackedxgbs0.15.csv')\n",
    "tabpfn_fixed17 = pd.read_csv('/kaggle/working/tabpfn_fixed0.17.csv')\n",
    "tabpfn14 = pd.read_csv('/kaggle/working/tabpfn14.csv')\n",
    "public_lgb15 = pd.read_csv('/kaggle/working/public_lgb15.csv')\n",
    "\n",
    "# Combine the 'Class_0' columns from all dataframes\n",
    "combined_class_0 = pd.concat([lgbm16['Class_0'], public_lgb15['class_0'], tabpfn14['class_0'], stackedxgbs15['class_0'], tabpfn_fixed17['class_0']], axis=1)\n",
    "\n",
    "# Calculate the weighted ensemble based on inverse of log losses\n",
    "log_losses = {\n",
    "    'lgbm16': 0.16,\n",
    "    'stackedxgbs15': 0.15,\n",
    "    'tabpfn_fixed17': 0.17,\n",
    "    'tabpfn14': 0.14,\n",
    "    'public_lgb15': 0.15\n",
    "}\n",
    "\n",
    "# Calculate weights based on the inverse of log loss\n",
    "weights = [1 / loss for loss in log_losses.values()]\n",
    "total_weight = sum(weights)\n",
    "weights = [weight / total_weight for weight in weights]\n",
    "\n",
    "weighted_class_0 = (combined_class_0 * weights).sum(axis=1) / len(log_losses)\n",
    "\n",
    "# Apply post-processing to 'Class_0'\n",
    "# weighted_class_0[(weighted_class_0 < 0.02)] = 0\n",
    "# weighted_class_0[(weighted_class_0 > 0.95)] = 1\n",
    "\n",
    "# Calculate 'Class_1' as 1 - 'Class_0'\n",
    "weighted_class_1 = 1 - weighted_class_0\n",
    "\n",
    "# Create a new dataframe for submission\n",
    "submission_df = pd.DataFrame({'Id': lgbm16['Id'], 'Class_0': weighted_class_0, 'Class_1': weighted_class_1})\n",
    "\n",
    "# Apply post-processing to 'Class_0' and 'Class_1' predictions based on 'test.csv'\n",
    "for i, row in test.iterrows():\n",
    "    if row['BN'] <= 44:  # Chapter 1 finding\n",
    "        submission_df.loc[i, 'Class_0'] = 1\n",
    "        submission_df.loc[i, 'Class_1'] = 0\n",
    "    if pd.isnull(row['BQ']):  # Chapter 2 finding\n",
    "        submission_df.loc[i, 'Class_0'] = 1\n",
    "        submission_df.loc[i, 'Class_1'] = 0\n",
    "\n",
    "# Save the updated dataframe as submission.csv\n",
    "# submission_df.to_csv('submission.csv', index=False)\n",
    "weighted_class_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0e2f552",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T15:16:23.502252Z",
     "iopub.status.busy": "2023-08-09T15:16:23.501848Z",
     "iopub.status.idle": "2023-08-09T15:16:23.510091Z",
     "shell.execute_reply": "2023-08-09T15:16:23.508506Z"
    },
    "papermill": {
     "duration": 0.053793,
     "end_time": "2023-08-09T15:16:23.512485",
     "exception": false,
     "start_time": "2023-08-09T15:16:23.458692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"/kaggle/working/\"\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# Delete CSV files\n",
    "for file in files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        os.remove(file_path)\n",
    "\n",
    "print(\"CSV files deleted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af6674ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T15:16:23.600116Z",
     "iopub.status.busy": "2023-08-09T15:16:23.599008Z",
     "iopub.status.idle": "2023-08-09T15:16:23.614919Z",
     "shell.execute_reply": "2023-08-09T15:16:23.613648Z"
    },
    "papermill": {
     "duration": 0.062889,
     "end_time": "2023-08-09T15:16:23.617707",
     "exception": false,
     "start_time": "2023-08-09T15:16:23.554818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Class_0</th>\n",
       "      <th>Class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id  Class_0  Class_1\n",
       "0  00eed32682bb      1.0      0.0\n",
       "1  010ebe33f668      1.0      0.0\n",
       "2  02fa521e1838      1.0      0.0\n",
       "3  040e15f562a2      1.0      0.0\n",
       "4  046e85c7cc7f      1.0      0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the updated dataframe as submission.csv\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2df727",
   "metadata": {
    "papermill": {
     "duration": 0.044189,
     "end_time": "2023-08-09T15:16:23.707543",
     "exception": false,
     "start_time": "2023-08-09T15:16:23.663354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 867.568696,
   "end_time": "2023-08-09T15:16:26.977287",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-09T15:01:59.408591",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
